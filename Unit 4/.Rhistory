View(concrete)
plot(concrete$flyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh, pch = 19)
plot(concrete$Age ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 6)
table(cutBlast)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutFly <- cut2(training$FlyAsh, g = 6)
table(cutFly)
cutFly <- cut2(training$FlyAsh, g = 7)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
cutAge <- cut2(training$Age, g = 6)
table(cutAge)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 7)
table(cutBlast)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
qplot(cutAge, CompressiveStrength, data = training, fill = cutAge, geom = c('boxplot', 'jitter'))
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
install.packages("iris")
library(iris)
samConcrete <- concrete[200:300, ]
plot(samConcrete$CompressiveStrength ~ samConcrete$Age, pch = 19)
samConcrete0 <- concrete[100:200, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
samConcrete0 <- concrete[1:100, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
samConcrete1 <- concrete[101:200, ]
plot(samConcrete1$CompressiveStrength ~ samConcrete1$Age, pch = 19)
samConcrete2 <- concrete[201:300, ]
plot(samConcrete2$CompressiveStrength ~ samConcrete2$Age, pch = 19)
samConcrete3 <- concrete[301:400, ]
plot(samConcrete3$CompressiveStrength ~ samConcrete3$Age, pch = 19)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(training$Superplasticizer, xlim = c(0, 0.0001))
hist(training$Superplasticizer, xlim = c(0, 0.001))
hist(training$Superplasticizer, xlim = c(0, 0.01))
hist(training$Superplasticizer, xlim = c(0, 0.0001), bin = 10)
qplot(training$Superplasticizer)
qplot(training$Superplasticizer, xlim = c(0, 0.001))
qplot(log(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
adData
View(adData)
str(adData)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
which(LETTERS == 'R')
which((1:12) %% 2 == 0)
qt(0.05/2, 7)
source('~/Dropbox/Coursera/Practical Machine Learning/Project/Project.R')
install.packages("rattle")
source('~/.active-rstudio-document')
library(caret)
library(ggplot2)
library(gridExtra)
library(rpart)
library(randomForest)
library(rattle)
library(doMC)
registerDoMC(cores = 4)
# The data is imported from hard drive and into to R memory. The data can also be imported directly from the URL as well.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv")
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv")
# Inspecting the data, several column vectors that are empty and with NA values are found. This can be dealt with by specifying na.strings during import itself.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv", na.strings = c(NA, '', '#DIV/0!'))
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv", , na.strings = c(NA, '', '#DIV/0!'))
dim(training); dim(testing)
# Examining the data suggests there are still columns with high percentage of NA values
training <- training[ , (nrow(training) - colSums(is.na(training)))/nrow(training) > 0.9]
testing  <-  testing[ , (nrow(testing) - colSums(is.na(testing)))/nrow(testing) > 0.9]
dim(training); dim(testing)
# Plot the interactions by user, by classe.
grid_arrange_shared_legend <- function(...) {
plots <- list(...)
g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
lheight <- sum(legend$height)
grid.arrange(
do.call(arrangeGrob, lapply(plots, function(x)
x + theme(legend.position="none"))),
legend,
ncol = 1,
heights = unit.c(unit(1, "npc") - lheight, lheight))
}
plot1 <- ggplot(data = training, aes(x = user_name, y = total_accel_belt)) + geom_point(aes(colour = classe))
plot2 <- ggplot(data = training, aes(x = user_name, y = total_accel_arm)) + geom_point(aes(colour = classe))
plot3 <- ggplot(data = training, aes(x = user_name, y = total_accel_dumbbell)) + geom_point(aes(colour = classe))
plot4 <- ggplot(data = training, aes(x = user_name, y = total_accel_forearm)) + geom_point(aes(colour = classe))
grid_arrange_shared_legend(plot1, plot2, plot3, plot4)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_forearm, y = pitch_forearm )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_dumbbell, y = magnet_dumbbell_z )) + geom_point() + facet_wrap(~classe, nrow =1)
# Removing the columns that will not be used in prediction
myTraining <- training[ , 8:60]
mTesting <- testing[ , 8:60]
dim(training); dim(testing)
# Convert the response variable to a factor variable, to be predicted
training$classe <- factor(training$classe)
# Create models for prediction:
# The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable
# in the training set. You may use any of the other variables to predict with. You should create a report describing
# how you built your model, how you used cross validation, what you think the expected out of sample error is, and why
# you made the choices you did. You will also use your prediction model to predict 20 different test cases.
# caret allows us to specify a trainControl function that can be applied across all model predictions. Lets
# first define the trainControl parameter
# Lets use a 5-fold cross-validation
trnCtrl <- trainControl(method = 'cv',
number = 5,
allowParallel = TRUE,
verboseIter = TRUE)
model1 <- train(classe ~ .,
data = training,
method = 'rf',
trControl = trnCtrl)
model2 <- train(classe ~ .,
data = training,
method = 'knn',
trControl = trnCtrl)
# c("roll_belt", "pitch_belt", "yaw_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "",
#  "accel_belt_y", "accel_belt_z", "magnet_belt_x",  "magnet_belt_y", "magnet_belt_z")
#
# pml_write_files = function(x){
#   n = length(x)
#   for(i in 1:n){
#     filename = paste0("problem_id_",i,".txt")
#     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#   }
# }
#
# x <- evaluation_data
# x <- x[feature_set[feature_set!='classe']]
# answers <- predict(rf, newdata=x)
#
# answers
#
# pml_write_files(answers)
library(ggplot2)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = myTraining, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
library(ggplot)
library(ggplot2)
library(knitr)
install.packages("knitr")
qf(0.95, df1 = 19, df2 = 24)
4.405/0.643
qf(0.95, df1 = 2, df2 = 36)
19*3
117-57
60/24
19/2.5
60/21
19/2.857
library(SDSFoundations)
?data
data()
film <- data(FilmData)
str(film)
film <- FilmData
View(film)
fivenum(film$Days)
boxplot(film$Days, main = 'Days in Theaters', xlab = 'All Films', ylab = '# of days')
hist(film$Days)
boxplot(film$Days ~ film$Genre, main = "Days in Theaters", xlab = 'Genrea', ylab = '# of days')
aggregate(Days ~ Genre, data = film)
aggregate(Days ~ Genre, film)
aggregate(Days ~ Genre, film, mean)
?aggregate
aggregate(Days ~ Genre, film, sd)
daysModel <- aov(film$Days ~ film$Genre)
summary(daysModel)
TukeyHSD(daysModel)
library(MASS)
library(ISLR)
### Simple linear regression
names(Boston)
?Boston
plot(medv~lstat,Boston)
View(Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
`###Qualitative predictors
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
gerber <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 4/gerber.csv")
str(gerber)
prop.table(as.matrix(table(gerber$voting)))
hawthorne = subset(gerber, hawthorne == 1)
table(hawthorne$voting)
table(gerber$voting, gerber$hawthorne)
table(gerber$voting, gerber$hawthorne)[2, 2]
prop.table(as.matrix(table(hawthorne$voting))
prop.table(as.matrix(table(hawthorne$voting)))
str(gerber)
civicduty = subset(gerber, civicduty == 1)
prop.table(as.matrix(table(civicduty$voting)))
neighbors = subset(gerber, neighbors == 1)
prop.table(as.matrix(table(neighbors$voting)))
self = subset(gerber, self == 1)
prop.table(as.matrix(table(self$voting))
)
setwd("~/Dropbox/EdX/The Analytics Edge/Unit 4")
library(caret)
library(e1701)
library(e1071)
library(rpart)
library(rpart.plot)
library(caTools)
library(ROCR)
library(randomforest)
library(randomForest)
gerberLR = glm(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, family="binomial")
summary(gerberLR)
table(gerber$voting, gerberLR)
table(gerberLR, gerber$voting)
gerber.predict.LR = predict(gerberLR, gerber, type = 'response')
table(gerber$voting, p >= 0.3)
table(gerber$voting, gerber.predict.LR >= 0.3)
(51966+134513)/(nrow(gerber))
table(gerber$voting, gerber.predict.LR >= 0.5)
108696/nrow(gerber)
235388/nrow(gerber)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel)
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
(1 - 0.38)*(1 - 0.35)*(1 - 0.32)*).3
(1 - 0.38)*(1 - 0.35)*(1 - 0.32)*0.3
CARTmodel3 = rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp=0.0)
prp(CARTmodel3)
CARTmodelControl = rpart(voting ~ control, data=gerber, cp=0.0)
prp(CARTmodelControl, digits=6)
abs(0.296638-0.34)
CARTmodelControl = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(CARTmodelControl, digits=6)
CARTmodelControl = rpart(voting ~ control, data=gerber, cp=0.0)
CARTmodelSex = rpart(voting ~ control + sex, data=gerber, cp=0.0)
prp(CARTmodelSex, digits=6)
prp(CARTmodelControl, digits=6)
abs(0.290456-0.302795)
abs(0.334176-0.345818)
gerberLR2 = glm(voting ~ control + sex, data=gerber, family="binomial")
summary(gerberLR2)
Possibilities = data.frame(sex=c(0,0,1,1),control=c(0,1,0,1))
predict(gerberLR2, newdata=Possibilities, type="response")
Possibilities
predict(CARTmodelSex, newdata=Possibilities)
abs(0.2908065 - 0.2904558)
LogModel2 = glm(voting ~ sex + control + sex:control, data=gerber, family="binomial")
summary(LogModel2)
predict(LogModel2, newdata=Possibilities, type="response")
abs(0.2904558 - 0.2904558)
letters <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 4/letters_ABPR.csv", header=FALSE)
View(letters)
str(letters)
letters$isB = as.factor(letters$letter == "B")
letters <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 4/letters_ABPR.csv", header=TRUE)
str(letters)
letters$isB = as.factor(letters$letter == "B")
str(letters)
set.seed(1000)
split = sample.split(letter$isB, SplitRation = 0.5)
split = sample.split(letter$isB, SplitRatio = 0.5)
split = sample.split(letters$isB, SplitRatio = 0.5)
lettersTest = subset(letters, split == TRUE)
lettersTrain= subset(letters, split == TRUE)
lettersTest  = subset(letters, split == FALSE)
baseline = rep(FALSE, nrow(lettersTest))
table(lettersTest$isB, baseline)
1175/nrow(lettersTest)
CARTb = rpart(isB ~ . - letter, data=train, method="class")
CARTb = rpart(isB ~ . - letters, data=train, method="class")
str(letters)
CARTb = rpart(isB ~ . - letter, data=lettersTrain, method="class")
p = predict(CARTb, newdata = lettersTest, type="class")
c =table(lettersTest$isB, p)
c
(c[1,1]+c[2,2])/sum(c)
RFb = randomForest(isB ~ . - letter, data = lettersTrain)
summary(CARTb)
prb(CARTb)
prp(CARTb)
p = predict(RFb,
newdata = lettersTest,
type="class")
c = table(lettersTest$letter, p)
(c[1,1]+c[2,2]+c[3,3]+c[4,4])/nrow(Test)
c = table(lettersTest$letter, p)
c
RFb = randomForest(letter ~ . - isB,
data = lettersTrain)
p1 = predict(RFb,
newdata = lettersTest,
type="class")
c = table(lettersTest$letter, p1)
c
RFb = randomForest(isB ~ . - letter,
data=lettersTrain)
p = predict(RFb,
newdata=lettersTest,
type="class")
c = table(Test$isB, p)
c
c = table(lettersTest$isB, p)
c
(c[1,1]+c[2,2])/sum(c)
letters$letter = as.factor( letters$letter )
table(letters$letter)
set.seed(2000 )
split = sample.split(letters$letter, SplitRatio = 0.5)
train = subset(letters, split == TRUE)
test = subset(letters, split == FALSE)
table(train$letter)
baseline = repeat(P, nrow(train))
baseline = repeat("P", nrow(train))
baseline = repeat('P', nrow(train))
baseline = rep('P', nrow(train))
baseline
baseline = rep(P, nrow(train))
baseline = rep('P', nrow(train))
table(letters$letter, baseline)
table(train$letter, baseline)
402/nrow(train)
CART = rpart(letter ~ . - isB,
data = train,
type = 'class')
CART = rpart(letter ~ . - isB,
data = train)
prp(CART)
CART = rpart(letter ~ . - isB,
data = train,
type = 'class')
CART = rpart(letter ~ . - isB,
data = train)
p = predict(CART,
newdata = test,
type = 'class')
p
table(test$letter, p)
(348+318+363+340)/sum(table(test$letter, p))
RF = randomForest(letter ~ . - isB,
data = train)
p = predict(RF,
newdata = test,
type = 'class')
table(test$letter, p)
(391+380+393+365)/sum(table(test$letter, p))
plot(RF)
plot(RF, type = 'l')
census <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 4/census.csv")
View(census)
str(census)
set.seed(2000)
split = sample.split(census$over50k, SplitValue = 0.5)
split = sample.split(census$over50k, SplitRatio = 0.5)
train = subset(census, split == TRUE)
test = subset(census !split)
test = subset(census split == FALSE)
test = subset(census, split == FALSE)
logCensus = glm(over50k ~ .,
data = train,
type = 'binomial')
logCensus = glm(over50k ~ .,
data = train,
family = 'binomial')
split = sample.split(census$over50k, SplitRatio = 0.6)
train = subset(census, split == TRUE)
test = subset(census, split == FALSE)
logCensus = glm(over50k ~ .,
data = train,
family = 'binomial')
predCensus = predict(logCensus,
newdata = test)
p = predict(logCensus,
newdata = test)
c = table(test$over50k, p >= 0.5)
c
table(test$over50k)
summary(logCensus)
c
(c[1,1]+c[2,2])/sum(c)
set.seed(2000)
spl = sample.split(census$over50k, SplitRatio=0.6)
Train = subset(census, spl)
Test = subset(census, !spl)
censusLR = glm(over50k ~ ., data=Train, family="binomial")
summary(censusLR)
table(Train$over50k)
pbase = rep("<=50K", nrow(Test))
c =table(Test$over50k, pbase)
(c[1,1])/sum(c)
p = predict(censusLR, newdata=Test)
c =table(Test$over50k, p>=0.5)
(c[1,1]+c[2,2])/sum(c)
c
table(Train$over50k)
pbase = rep("<=50K", nrow(Test))
c =table(Test$over50k, pbase)
(c[1,1])/sum(c)
pred = prediction(p, Test$over50k)
perf = performance(pred, "tpr", "fpr")
plot(perf)
as.numeric(performance(pred, "auc")@y.values)
censusRT = rpart(over50k ~ ., data=Train, method="class")
prp(censusRT)
p2 = predict(censusRT, newdata=Test)[,2]
c =table(Test$over50k, p2>=0.5)
(c[1,1]+c[2,2])/sum(c)
pred = prediction(p2, Test$over50k)
perf = performance(pred, "tpr", "fpr")
plot(perf)
as.numeric(performance(pred, "auc")@y.values)
set.seed(1)
trainSmall = Train[sample(nrow(Train), 2000), ]
censusRF = randomForest(over50k ~ ., data=trainSmall, method="class")
p3 = predict(censusRF, newdata=Test)
c =table(Test$over50k, p3)
(c[1,1]+c[2,2])/sum(c)
c =table(Test$over50k, p3 >= 0.5)
p3
c =table(Test$over50k, p3)
(c[1,1]+c[2,2])/sum(c)
vu = varUsed(censusRF, count=TRUE)
vusorted = sort(vu, decreasing = FALSE, index.return = TRUE)
dotchart(vusorted$x, names(censusRF$forest$xlevels[vusorted$ix]))
varImpPlot(censusRF)
set.seed(2)
cartGrid = expand.grid( .cp = seq(0.002,0.1,0.002))
numFolds = trainControl(method="cv", number=10)
train(over50k ~ ., data=Train, method="rpart", trControl=numFolds, tuneGrid = cartGrid)
censusRF2 = rpart(over50k ~ ., data=Train, method="class", cp=0.002)
p3 = predict(censusRF2, newdata=Test, type="class")
c =table(Test$over50k, p3)
(c[1,1]+c[2,2])/sum(c)
prp(censusRF2)
summary(censusRF2)
(c[1,1]+c[2,2])/sum(c)
statedata <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 4/statedataSimple.csv")
View(statedata)
str(statedata)
