plot = 'pairs')
qplot(age, wage, data = training)
qplot(age, wage, data = training, color = age)
qplot(age, wage, data = training, color = jobclass)
qplot(age, wage, data = training, color = education)
modFit <- train(wage ~ age + jobclass + education,
method = 'lm',
data = training)
finMod <- modFit$finalModel
finMod
print(modFit)
plot(finMod, 1, pch = 19, cex = 0.5, col = 'blue')
qplot(finMod$fitted, finMod$residuals, colour = race, data = training)
plot(finMod$residuals, pch = 19)
pred <- predict(modFit, testing)
qplot(wage, pred, colour = year, data = testing)
par(mfrow = c(1,1))
plot(finMod$residuals, pch = 19)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
str(concrete)
qplot(CompressiveStrength, data = concrete)
plot(concrete$CompressiveStrength, pch = 19)
featurePlot(x = concrete[ , c('Cement', 'BlastFurnaceSlag', 'FlyAsh', 'Water',
'Superplasticizer', 'CoarseAggregate',
'FineAggregate', 'Age')],
y = concrete$CompressiveStrength,
plot = 'pairs')
cutFly <- cut2(taining$FlyAsh, g = 5)
cutFly <- cut2(training$FlyAsh, g = 5)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot'))
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
View(concrete)
plot(concrete$flyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh, pch = 19)
plot(concrete$Age ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 6)
table(cutBlast)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutFly <- cut2(training$FlyAsh, g = 6)
table(cutFly)
cutFly <- cut2(training$FlyAsh, g = 7)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
cutAge <- cut2(training$Age, g = 6)
table(cutAge)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 7)
table(cutBlast)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
qplot(cutAge, CompressiveStrength, data = training, fill = cutAge, geom = c('boxplot', 'jitter'))
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
install.packages("iris")
library(iris)
samConcrete <- concrete[200:300, ]
plot(samConcrete$CompressiveStrength ~ samConcrete$Age, pch = 19)
samConcrete0 <- concrete[100:200, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
samConcrete0 <- concrete[1:100, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
samConcrete1 <- concrete[101:200, ]
plot(samConcrete1$CompressiveStrength ~ samConcrete1$Age, pch = 19)
samConcrete2 <- concrete[201:300, ]
plot(samConcrete2$CompressiveStrength ~ samConcrete2$Age, pch = 19)
samConcrete3 <- concrete[301:400, ]
plot(samConcrete3$CompressiveStrength ~ samConcrete3$Age, pch = 19)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(training$Superplasticizer, xlim = c(0, 0.0001))
hist(training$Superplasticizer, xlim = c(0, 0.001))
hist(training$Superplasticizer, xlim = c(0, 0.01))
hist(training$Superplasticizer, xlim = c(0, 0.0001), bin = 10)
qplot(training$Superplasticizer)
qplot(training$Superplasticizer, xlim = c(0, 0.001))
qplot(log(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
adData
View(adData)
str(adData)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
which(LETTERS == 'R')
which((1:12) %% 2 == 0)
qt(0.05/2, 7)
source('~/Dropbox/Coursera/Practical Machine Learning/Project/Project.R')
install.packages("rattle")
source('~/.active-rstudio-document')
library(caret)
library(ggplot2)
library(gridExtra)
library(rpart)
library(randomForest)
library(rattle)
library(doMC)
registerDoMC(cores = 4)
# The data is imported from hard drive and into to R memory. The data can also be imported directly from the URL as well.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv")
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv")
# Inspecting the data, several column vectors that are empty and with NA values are found. This can be dealt with by specifying na.strings during import itself.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv", na.strings = c(NA, '', '#DIV/0!'))
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv", , na.strings = c(NA, '', '#DIV/0!'))
dim(training); dim(testing)
# Examining the data suggests there are still columns with high percentage of NA values
training <- training[ , (nrow(training) - colSums(is.na(training)))/nrow(training) > 0.9]
testing  <-  testing[ , (nrow(testing) - colSums(is.na(testing)))/nrow(testing) > 0.9]
dim(training); dim(testing)
# Plot the interactions by user, by classe.
grid_arrange_shared_legend <- function(...) {
plots <- list(...)
g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
lheight <- sum(legend$height)
grid.arrange(
do.call(arrangeGrob, lapply(plots, function(x)
x + theme(legend.position="none"))),
legend,
ncol = 1,
heights = unit.c(unit(1, "npc") - lheight, lheight))
}
plot1 <- ggplot(data = training, aes(x = user_name, y = total_accel_belt)) + geom_point(aes(colour = classe))
plot2 <- ggplot(data = training, aes(x = user_name, y = total_accel_arm)) + geom_point(aes(colour = classe))
plot3 <- ggplot(data = training, aes(x = user_name, y = total_accel_dumbbell)) + geom_point(aes(colour = classe))
plot4 <- ggplot(data = training, aes(x = user_name, y = total_accel_forearm)) + geom_point(aes(colour = classe))
grid_arrange_shared_legend(plot1, plot2, plot3, plot4)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_forearm, y = pitch_forearm )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_dumbbell, y = magnet_dumbbell_z )) + geom_point() + facet_wrap(~classe, nrow =1)
# Removing the columns that will not be used in prediction
myTraining <- training[ , 8:60]
mTesting <- testing[ , 8:60]
dim(training); dim(testing)
# Convert the response variable to a factor variable, to be predicted
training$classe <- factor(training$classe)
# Create models for prediction:
# The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable
# in the training set. You may use any of the other variables to predict with. You should create a report describing
# how you built your model, how you used cross validation, what you think the expected out of sample error is, and why
# you made the choices you did. You will also use your prediction model to predict 20 different test cases.
# caret allows us to specify a trainControl function that can be applied across all model predictions. Lets
# first define the trainControl parameter
# Lets use a 5-fold cross-validation
trnCtrl <- trainControl(method = 'cv',
number = 5,
allowParallel = TRUE,
verboseIter = TRUE)
model1 <- train(classe ~ .,
data = training,
method = 'rf',
trControl = trnCtrl)
model2 <- train(classe ~ .,
data = training,
method = 'knn',
trControl = trnCtrl)
# c("roll_belt", "pitch_belt", "yaw_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "",
#  "accel_belt_y", "accel_belt_z", "magnet_belt_x",  "magnet_belt_y", "magnet_belt_z")
#
# pml_write_files = function(x){
#   n = length(x)
#   for(i in 1:n){
#     filename = paste0("problem_id_",i,".txt")
#     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#   }
# }
#
# x <- evaluation_data
# x <- x[feature_set[feature_set!='classe']]
# answers <- predict(rf, newdata=x)
#
# answers
#
# pml_write_files(answers)
library(ggplot2)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = myTraining, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
library(ggplot)
library(ggplot2)
library(knitr)
install.packages("knitr")
qf(0.95, df1 = 19, df2 = 24)
4.405/0.643
qf(0.95, df1 = 2, df2 = 36)
19*3
117-57
60/24
19/2.5
60/21
19/2.857
library(SDSFoundations)
?data
data()
film <- data(FilmData)
str(film)
film <- FilmData
View(film)
fivenum(film$Days)
boxplot(film$Days, main = 'Days in Theaters', xlab = 'All Films', ylab = '# of days')
hist(film$Days)
boxplot(film$Days ~ film$Genre, main = "Days in Theaters", xlab = 'Genrea', ylab = '# of days')
aggregate(Days ~ Genre, data = film)
aggregate(Days ~ Genre, film)
aggregate(Days ~ Genre, film, mean)
?aggregate
aggregate(Days ~ Genre, film, sd)
daysModel <- aov(film$Days ~ film$Genre)
summary(daysModel)
TukeyHSD(daysModel)
library(MASS)
library(ISLR)
### Simple linear regression
names(Boston)
?Boston
plot(medv~lstat,Boston)
View(Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
`###Qualitative predictors
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
statedata <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 4/statedataSimple.csv")
str(statedata)
stateLM = glm(Life.Exp ~ ., data = statedata)
summary(stateLM)
stateLM = lm(Life.Exp ~ .,
data = statedata)
summary(stateLM)
pState = predict(stateLM,
newdata = stateLM)
pState = predict(stateLM)
SSE = (pState$Life.Exp - statedata$Life.Exp)^2/nrow(statedata)
SSE = sum((statedata$Life.Exp - pState)^2)
SSE
str(statedata)
stateLM1 = lm(Life.Exp ~ Population + Murder + HS.Grad + Frost,
data = statedata)
summary(stateLM1)
p1 = predict(stateLM1)
SSE1 = sum((statedata$Life.Exp - p1)^2)
SSE1
stateCART1 = glm(Life.Exp ~ .
data = statedata,
type = 'class')
load(caTools)
ls
ls()
library(caTools)
library("caret", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("randomForest", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("class", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("e1071", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library("doMC", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:doMC", unload=TRUE)
detach("package:foreach", unload=TRUE)
library("doMC", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
stateCART1 = glm(Life.Exp ~ .data = statedata, type = 'class')
stateCART1 = glm(Life.Exp ~ ., data = statedata, type = 'class')
stateCART1 = glm(Life.Exp ~ ., data = statedata)
summary(stateCART1)
prp(stateCART1)
library(cart.plot)
library(rpart)
library(rpart.plot)
prp(stateCART1)
plot(stateCART1)
stateCART2 = rpart(Life.Exp ~ .,
data = statedata,
method="class")
summary(stateCART2)
prp(stateCART2)
p3 = predict(stateCART2, newdata = statedate, type = 'class')
p3 = predict(stateCART2, newdata = statedata, type = 'class')
p3
SSE = sum((statedata$Life.Exp - p3)^2)
SSE
ps[1]
p3[1]
p3 = predict(stateCART2)
p3
SSE = sum((statedata$Life.Exp - p3)^2)
SSE
stateCART2 = rpart(Life.Exp ~ .,
data = statedata)
summary(stateCART2)
prp(stateCART2)
p3 = predict(stateCART2)
sum((statedata$Life.Exp - p3)^2)
stateCART3 = rpart(Life.Exp ~ .,
data = statedata,
minbucket = 5)
summary(stateCART3)
prp(stateCART3)
p3 = predict(stateCART3)
sum((statedata$Life.Exp - p3)^2)
stateCART4 = rpart(Life.Exp ~ .,
data = statedata,
minbucket = 1)
summary(stateCART4)
prp(stateCART4)
p4 = predict(stateCART4)
sum((statedata$Life.Exp - p4)^2)
stateCART4 = rpart(Life.Exp ~ Area,
data = statedata,
minbucket = 1)
summary(stateCART4)
prp(stateCART4)
p4 = predict(stateCART4)
sum((statedata$Life.Exp - p4)^2)
library(caret)
library(e1701)
library(e1071)
set.seed(111)
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .cp = seq(0.01,0.5,0.01))
train(Life.Exp ~ ., data = statedata, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
stateCART5 = rpart(Life.Exp ~ Area,
data = statedata,
method = 'class',
cp = 0.12)
summary(stateCART4)
prp(stateCART4)
summary(stateCART5)
prp(stateCART5)
stateCART5 = rpart(Life.Exp ~ .,
data = statedata,
method = 'class',
cp = 0.12)
summary(stateCART5)
prp(stateCART5)
stateCART5 = rpart(Life.Exp ~ .,
data = statedata,
cp = 0.12)
summary(stateCART5)
prp(stateCART5)
p4 = predict(stateCART5)
sum((statedata$Life.Exp - p4)^2)
train(Life.Exp ~ Area, data = statedata, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
set.seed(111)
train(Life.Exp ~ Area, data = statedata, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
stateCART6 = rpart(Life.Exp ~ .,
data = statedata,
cp = 0.12)
summary(stateCART6)
prp(stateCART6)
p6 = predict(stateCART6)
sum((statedata$Life.Exp - p6)^2)
stateCART6 = rpart(Life.Exp ~ .,
data = statedata,
cp = 0.02)
summary(stateCART6)
prp(stateCART6)
p6 = predict(stateCART6)
sum((statedata$Life.Exp - p6)^2)
set.seed(111)
train(Life.Exp ~ Area, data = statedata, method = "rpart", trControl = numFolds, tuneGrid = cpGrid )
stateCART6 = rpart(Life.Exp ~ .,
data = statedata,
cp = 0.02)
stateCART6 = rpart(Life.Exp ~ .,
data = statedata,
cp = 0.02)
summary(stateCART6)
prp(stateCART6)
stateCART6 = rpart(Life.Exp ~ Area,
data = statedata,
cp = 0.02)
summary(stateCART6)
prp(stateCART6)
p6 = predict(stateCART6)
sum((statedata$Life.Exp - p6)^2)
tweets <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 5/tweets.csv")
View(tweets)
str(tweets)
setwd("~/Dropbox/EdX/The Analytics Edge/Unit 5")
tweets$Negative = as.factor(tweets$Avg <= -1)
table(tweets$Negative)
install.packages("tm")
library(tm)
install.packages("SnowballC")
library(SnowballC)
corpus = Corpus(VectorSource(tweets$Tweet))
# Look at corpus
corpus
corpus[[1]]
# Convert to lower-case
corpus = tm_map(corpus, tolower)
corpus[[1]]
# IMPORTANT NOTE: If you are using the latest version of the tm package, you will need to run the following line before continuing (it converts corpus to a Plain Text Document). This is a recent change having to do with the tolower function that occurred after this video was recorded.
corpus = tm_map(corpus, PlainTextDocument)
# Remove punctuation
corpus = tm_map(corpus, removePunctuation)
corpus[[1]]
# Look at stop words
stopwords("english")[1:10]
# Remove stopwords and apple
corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))
corpus[[1]]
# Stem document
corpus = tm_map(corpus, stemDocument)
corpus[[1]]
# Video 6
# Create matrix
frequencies = DocumentTermMatrix(corpus)
frequencies
# Look at matrix
inspect(frequencies[1000:1005,505:515])
# Check for sparsity
findFreqTerms(frequencies, lowfreq=20)
# Remove sparse terms
sparse = removeSparseTerms(frequencies, 0.995)
sparse
# Convert to a data frame
tweetsSparse = as.data.frame(as.matrix(sparse))
# Make all variable names R-friendly
View(tweetsSparse)
colnames(tweetsSparse) = make.names(colnames(tweetsSparse))
View(tweetsSparse)
# Add dependent variable
tweetsSparse$Negative = tweets$Negative
# Split the data
library(caTools)
set.seed(123)
split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)
> findFreqTerms(frequencies, lowfreq=100)
findFreqTerms(frequencies, lowfreq=100)
library(rpart)
library(rpart.plot)
tweetCART = rpart(Negative ~ ., data=trainSparse, method="class")
prp(tweetCART)
# Evaluate the performance of the model
predictCART = predict(tweetCART, newdata=testSparse, type="class")
table(testSparse$Negative, predictCART)
# Compute accuracy
(294+18)/(294+6+37+18)
# Baseline accuracy
