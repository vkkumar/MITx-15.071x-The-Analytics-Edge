library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
str(concrete)
qplot(CompressiveStrength, data = concrete)
plot(concrete$CompressiveStrength, pch = 19)
featurePlot(x = concrete[ , c('Cement', 'BlastFurnaceSlag', 'FlyAsh', 'Water',
'Superplasticizer', 'CoarseAggregate',
'FineAggregate', 'Age')],
y = concrete$CompressiveStrength,
plot = 'pairs')
cutFly <- cut2(taining$FlyAsh, g = 5)
cutFly <- cut2(training$FlyAsh, g = 5)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot'))
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
View(concrete)
plot(concrete$flyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh, pch = 19)
plot(concrete$Age ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 6)
table(cutBlast)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutFly <- cut2(training$FlyAsh, g = 6)
table(cutFly)
cutFly <- cut2(training$FlyAsh, g = 7)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
cutAge <- cut2(training$Age, g = 6)
table(cutAge)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 7)
table(cutBlast)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
qplot(cutAge, CompressiveStrength, data = training, fill = cutAge, geom = c('boxplot', 'jitter'))
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
install.packages("iris")
library(iris)
samConcrete <- concrete[200:300, ]
plot(samConcrete$CompressiveStrength ~ samConcrete$Age, pch = 19)
samConcrete0 <- concrete[100:200, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
samConcrete0 <- concrete[1:100, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
samConcrete1 <- concrete[101:200, ]
plot(samConcrete1$CompressiveStrength ~ samConcrete1$Age, pch = 19)
samConcrete2 <- concrete[201:300, ]
plot(samConcrete2$CompressiveStrength ~ samConcrete2$Age, pch = 19)
samConcrete3 <- concrete[301:400, ]
plot(samConcrete3$CompressiveStrength ~ samConcrete3$Age, pch = 19)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(training$Superplasticizer, xlim = c(0, 0.0001))
hist(training$Superplasticizer, xlim = c(0, 0.001))
hist(training$Superplasticizer, xlim = c(0, 0.01))
hist(training$Superplasticizer, xlim = c(0, 0.0001), bin = 10)
qplot(training$Superplasticizer)
qplot(training$Superplasticizer, xlim = c(0, 0.001))
qplot(log(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
adData
View(adData)
str(adData)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
which(LETTERS == 'R')
which((1:12) %% 2 == 0)
qt(0.05/2, 7)
source('~/Dropbox/Coursera/Practical Machine Learning/Project/Project.R')
install.packages("rattle")
source('~/.active-rstudio-document')
library(caret)
library(ggplot2)
library(gridExtra)
library(rpart)
library(randomForest)
library(rattle)
library(doMC)
registerDoMC(cores = 4)
# The data is imported from hard drive and into to R memory. The data can also be imported directly from the URL as well.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv")
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv")
# Inspecting the data, several column vectors that are empty and with NA values are found. This can be dealt with by specifying na.strings during import itself.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv", na.strings = c(NA, '', '#DIV/0!'))
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv", , na.strings = c(NA, '', '#DIV/0!'))
dim(training); dim(testing)
# Examining the data suggests there are still columns with high percentage of NA values
training <- training[ , (nrow(training) - colSums(is.na(training)))/nrow(training) > 0.9]
testing  <-  testing[ , (nrow(testing) - colSums(is.na(testing)))/nrow(testing) > 0.9]
dim(training); dim(testing)
# Plot the interactions by user, by classe.
grid_arrange_shared_legend <- function(...) {
plots <- list(...)
g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
lheight <- sum(legend$height)
grid.arrange(
do.call(arrangeGrob, lapply(plots, function(x)
x + theme(legend.position="none"))),
legend,
ncol = 1,
heights = unit.c(unit(1, "npc") - lheight, lheight))
}
plot1 <- ggplot(data = training, aes(x = user_name, y = total_accel_belt)) + geom_point(aes(colour = classe))
plot2 <- ggplot(data = training, aes(x = user_name, y = total_accel_arm)) + geom_point(aes(colour = classe))
plot3 <- ggplot(data = training, aes(x = user_name, y = total_accel_dumbbell)) + geom_point(aes(colour = classe))
plot4 <- ggplot(data = training, aes(x = user_name, y = total_accel_forearm)) + geom_point(aes(colour = classe))
grid_arrange_shared_legend(plot1, plot2, plot3, plot4)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_forearm, y = pitch_forearm )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_dumbbell, y = magnet_dumbbell_z )) + geom_point() + facet_wrap(~classe, nrow =1)
# Removing the columns that will not be used in prediction
myTraining <- training[ , 8:60]
mTesting <- testing[ , 8:60]
dim(training); dim(testing)
# Convert the response variable to a factor variable, to be predicted
training$classe <- factor(training$classe)
# Create models for prediction:
# The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable
# in the training set. You may use any of the other variables to predict with. You should create a report describing
# how you built your model, how you used cross validation, what you think the expected out of sample error is, and why
# you made the choices you did. You will also use your prediction model to predict 20 different test cases.
# caret allows us to specify a trainControl function that can be applied across all model predictions. Lets
# first define the trainControl parameter
# Lets use a 5-fold cross-validation
trnCtrl <- trainControl(method = 'cv',
number = 5,
allowParallel = TRUE,
verboseIter = TRUE)
model1 <- train(classe ~ .,
data = training,
method = 'rf',
trControl = trnCtrl)
model2 <- train(classe ~ .,
data = training,
method = 'knn',
trControl = trnCtrl)
# c("roll_belt", "pitch_belt", "yaw_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "",
#  "accel_belt_y", "accel_belt_z", "magnet_belt_x",  "magnet_belt_y", "magnet_belt_z")
#
# pml_write_files = function(x){
#   n = length(x)
#   for(i in 1:n){
#     filename = paste0("problem_id_",i,".txt")
#     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#   }
# }
#
# x <- evaluation_data
# x <- x[feature_set[feature_set!='classe']]
# answers <- predict(rf, newdata=x)
#
# answers
#
# pml_write_files(answers)
library(ggplot2)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = myTraining, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
library(ggplot)
library(ggplot2)
library(knitr)
install.packages("knitr")
qf(0.95, df1 = 19, df2 = 24)
4.405/0.643
qf(0.95, df1 = 2, df2 = 36)
19*3
117-57
60/24
19/2.5
60/21
19/2.857
library(SDSFoundations)
?data
data()
film <- data(FilmData)
str(film)
film <- FilmData
View(film)
fivenum(film$Days)
boxplot(film$Days, main = 'Days in Theaters', xlab = 'All Films', ylab = '# of days')
hist(film$Days)
boxplot(film$Days ~ film$Genre, main = "Days in Theaters", xlab = 'Genrea', ylab = '# of days')
aggregate(Days ~ Genre, data = film)
aggregate(Days ~ Genre, film)
aggregate(Days ~ Genre, film, mean)
?aggregate
aggregate(Days ~ Genre, film, sd)
daysModel <- aov(film$Days ~ film$Genre)
summary(daysModel)
TukeyHSD(daysModel)
library(MASS)
library(ISLR)
### Simple linear regression
names(Boston)
?Boston
plot(medv~lstat,Boston)
View(Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
`###Qualitative predictors
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
OCCPerf = data.frame()
OCCPerf = edit(OCCPerf, factor.mode = "numeric")
X11()
ls
ls()
library(pROC)
install.packages("pROC")
library(pROC)
titanicDF <- read.csv('http://math.ucdenver.edu/RTutorial/titanic.txt',sep='\t')
titanicDF$Title <- ifelse(grepl('Mr ',titanicDF$Name),'Mr',ifelse(grepl('Mrs ',titanicDF$Name),'Mrs',ifelse(grepl('Miss',titanicDF$Name),'Miss','Nothing')))
# load libraries
library(caret)
library(pROC)
titanicDF <- read.csv('http://math.ucdenver.edu/RTutorial/titanic.txt',sep='\t')
?ifelse
install.packages("pROC")
library(pROC)
ls()
tcltk::View(titanicDF)
c(1:6)
install.packages("mlbench")
library(mlbench)
data("Sonar")
library(dplyr)
glimpse(Sonar)
library(caret)
set.seed(998)
inTraining = createDataPartition(Sonar$Class, p = 0.75)
inTraining = createDataPartition(Sonar$Class, p = 0.75, list = FALSE)
View(inTraining)
training = Sonar[ inTraining, ]
testing  = Sonar[-inTraining, ]
fitControl = trainControl(method = "repeatedCV",
number = 10,
repeats = 10) # 10-fold CV, repeated 10 times!
fitControl
gbmFit1 = train(Class ~ ., data = training,
method = 'gbm',
trControl = fitControl,
verbose = FALSE)
gbmFit1
summary(gbmFit1)
gbmGrid = expand.grid(interaction.depth = c(1, 5, 9),
n.trees = (1:30)*50,
shrinkage = 0.1,
n.minobsinnode = 20)
nrow(gbmGrid)
set.seed(825)
gbmGrid
gbmFit2 = train(Class ~ ., data = training,
method = 'gbm',
trControl = fitControl,
tuneGrid = gbmGrid)
summary(gbmFit2)
gbmFit2
trellis.par.set(caretTheme())
plot(gbmFit1)
plot(gbmFit2)
plot(gbmFit2, metric = "Kappa")
?predict
load("~/Dropbox/EdX/The Analytics Edge/Final/Movies/Movies.csv")
a = [1, 2]
a <- [1.2]
a <- c(1,2)
a
a[1]
a[2]
c = c(3, 4)
a.append(c)
a.join(c)
a = a + c
a
c
?list.dirs
list.dirs()
setwd("~/Dropbox/EdX/The Analytics Edge/Final/Retail Consumers")
Households <- read.csv("Households.csv", stringsAsFactors=FALSE)
str(Households)
summary(Households$MorningPct)
hist(Households$MorningPct, breaks = 20)
library(corrplot)
library(rattle)
library(plyr)
library(ggplot2)
# How many households have logged transactions at the retailer only
# in the morning?
nrow(Households[Households$MorningPct == 100, ])
# How many households have logged transactions at the retailer only
# in the afternoon?
nrow(Households[Households$AfternoonPct == 100, ])
# Of the households that spend more than $150 per transaction on average, what
# is the minimum average discount per transaction?
min(Households[Households$AvgSalesValue > 150, ]$AvgDiscount)
# Of the households who have an average discount per transaction greater
# than 25%, what is the minimum average sales value per transaction?
min(Households[Households$AvgDiscount > 25, ]$AvgSalesValue)
# In the dataset, what proportion of households visited the retailer at least
# 300 times?
prop.table(table(Households$NumVisits >= 300))
# Normalizing the data
library(caret)
library(flexclust)
preproc = preProcess(Households)
HouseholdsNorm = predict(preproc, Households)
max(HouseholdsNorm$NumVisits)
min(HouseholdsNorm$AfternoonPct)
# Dendrogram of the data
set.seed(200)
distances <- dist(HouseholdsNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
# K-means Clustering
km = kmeans(HouseholdsNorm, centers = 10)
km$size
km$centers
clusterDF = data.frame(km$centers)
clusterDF$cluster = c(1:10)
arrange(clusterDF, desc(MorningPct - AvgDiscount))
arrange(clusterDF, desc(MorningPct), AvgDiscount)
arrange(clusterDF, desc(AvgProdCount + AvgSalesValue))
arrange(clusterDF, desc(AvgProdCount), AvgSalesValue)
# K-means with seed 5000
set.seed(5000)
km1 = kmeans(HouseholdsNorm, centers = 5)
km1$size
# Predict with flex
km1.kcca = as.kcca(km1, HouseholdsNorm)
clusterTrain = predict(km1.kcca)
table(clusterTrain)
cluster1 = subset(Households, clusterTrain == 1)
cluster2 = subset(Households, clusterTrain == 2)
cluster3 = subset(Households, clusterTrain == 3)
cluster4 = subset(Households, clusterTrain == 4)
cluster5 = subset(Households, clusterTrain == 5)
clusterDF = data.frame(km1$centers)
clusterDF$cluster = c(1:5)
arrange(clusterDF, desc(clusterDF$NumVisits), clusterDF$AvgDiscount)
setwd("~/Dropbox/EdX/The Analytics Edge/Final/Retail Consumers")
Households <- read.csv("Households.csv", stringsAsFactors=FALSE)
str(Households)
summary(Households$MorningPct)
hist(Households$MorningPct, breaks = 20)
library(corrplot)
library(rattle)
library(plyr)
library(ggplot2)
# How many households have logged transactions at the retailer only
# in the morning?
nrow(Households[Households$MorningPct == 100, ])
# How many households have logged transactions at the retailer only
# in the afternoon?
nrow(Households[Households$AfternoonPct == 100, ])
# Of the households that spend more than $150 per transaction on average, what
# is the minimum average discount per transaction?
min(Households[Households$AvgSalesValue > 150, ]$AvgDiscount)
# Of the households who have an average discount per transaction greater
# than 25%, what is the minimum average sales value per transaction?
min(Households[Households$AvgDiscount > 25, ]$AvgSalesValue)
# In the dataset, what proportion of households visited the retailer at least
# 300 times?
prop.table(table(Households$NumVisits >= 300))
# Normalizing the data
library(caret)
library(flexclust)
preproc = preProcess(Households)
HouseholdsNorm = predict(preproc, Households)
max(HouseholdsNorm$NumVisits)
min(HouseholdsNorm$AfternoonPct)
# Dendrogram of the data
set.seed(200)
distances <- dist(HouseholdsNorm, method = "euclidean")
ClusterShoppers <- hclust(distances, method = "ward.D")
plot(ClusterShoppers, labels = FALSE)
# K-means Clustering
km = kmeans(HouseholdsNorm, centers = 10)
km$size
km$centers
clusterDF = data.frame(km$centers)
clusterDF$cluster = c(1:10)
arrange(clusterDF, desc(MorningPct - AvgDiscount))
arrange(clusterDF, desc(MorningPct), AvgDiscount)
arrange(clusterDF, desc(AvgProdCount + AvgSalesValue))
arrange(clusterDF, desc(AvgProdCount), AvgSalesValue)
arrange(clusterDF, NumVisits, desc(AvgSalesValue))
arrange(clusterDF, NumVisits, AvgSalesValue)
arrange(clusterDF, desc(NumVisits), AvgSalesValue)
rattle()
ls
ls()
vars
a = list(c(1, 2))
a
a[1]
b = a[1]
b
a$new = c(3, 4, 5)
a
a$new
a(new1) = c(5, 6, 7)
a
a[2]
a[3] = lis(c('a', 'b'))
a[3] = list(c('a', 'b'))
a
a[[4]] = [1, 3, 4]
a
a$new
a$new[1]
a[[4]] = c(1, 2)
a
HouseholdDF = data.frame(HouseholdsNorm, cluster = km$cluster)
str(HouseholdDF)
corrplot(HouseholdDF)
str(HouseholdDF)
corrplot(HouseholdDF[1:6])
HouseholdsNormCor = cor(HouseholdsNorm)
HouseholdsNormCor
data(mtcars)
mtcars
str(mtcars)
mtcars$mpg
mtcars[["mpg"]]
mtcars[1]
mtcars[1,3]
mtcars[1]
data(mtcars)
c = cor(mtcars)
corrplot(c)
str(mtcars)
(order.AOE <- corrMatOrder(M, order="AOE"))
(order.AOE <- corrMatOrder(c, order="AOE"))
c.AOE = c[order.AOE, order.AOE]
corrplot(c.AOE)
(order.FPC <- corrMatOrder(c, order="FPC"))
c.FPC = c[order.FPC, order.FPC]
corrplot(order.FPC)
View(c.AOE)
