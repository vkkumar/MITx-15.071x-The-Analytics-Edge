set.seed(32343)
modelFit <- train(type ~ ., data = training, preProcess = c('center', 'scale'),
method = 'glm')
modelFit
preObj <- preProcess(training[ ,-58], method = c('BoxCox'))
trainCapAveS <- predict(preObj, training[ ,-58])$capitalAve
par(mfrow = c(1, 2)); hist(trainCapAveS); qqnorm(trainCapAveS)
set.seed(13343)
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size = 1, prob = 0.5) == 1
training$capAve[selectNA] <- NA
preObj <- preProcess(training[, -58], method = 'knnImpute')
capAve <- predict(preObj, training[ , -58])$capAve
install.packages("RANN")
library(RANN)
capAve <- predict(preObj, training[ , -58])$capAve
preObj <- preProcess(training[, -58], method = 'knnImpute')
capAve <- predict(preObj, training[ , -58])$capAve
capAveTruth <- training$capitalAve
capAveTruth <-(capAveTruth - mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
quantile(capAve - capAveTruth)[selectNA]
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
library(ISLR); library(caret); data(Wage)
data(Wage)
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain, ]
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex = 0.5)
points(training$age, predict(lm1, newdata = training), col = 'red',
pch = 19, cex = 0.5)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-ingTrain, ]
testing <- spam[-inTrain, ]
M <- abs(cor(training[ , -58]))
diag(M) <- 0
which(M > 0.8, arr.ind = T)
names(spam)[c(34, 32)]
plot(spam[, 34], spam[ , 32])
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
training
str(training)
summary(cars)
```{r}
library(ISLR); library(ggplot2); library(caret)
data(Wage); Wage <- subset(Wage, select = - c(logwage))
summary(Wage)
str(Wage)
logwage
c(logwage)
data(Wage); Wage <- subset(Wage, select = - c(log(wage)))
summary(Wage)
data(Wage); Wage <- subset(Wage, select = - c(logwage))
summary(Wage)
data(Wage); Wage <- subset(Wage, select = - c(log(wage)))
summary(Wage)
data(Wage); Wage <- subset(Wage, select = - c(logwage))
summary(Wage)
str(Wage)
inTrain <- createDataPartion(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain, ]
inTrain <- createDataPartion(y = Wage$wage, p = 0.7, list = FALSE)
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]; testing <- Wage[-inTrain, ]
dim(training); dim(testing)
featurePlot(x = training[ , c('age', 'education', 'jobclass')],
y = training$wage,
plot = 'pairs')
qplot(age, wage, data = training)
qplot(age, wage, data = training, color = age)
qplot(age, wage, data = training, color = jobclass)
qplot(age, wage, data = training, color = education)
modFit <- train(wage ~ age + jobclass + education,
method = 'lm',
data = training)
finMod <- modFit$finalModel
finMod
print(modFit)
plot(finMod, 1, pch = 19, cex = 0.5, col = 'blue')
qplot(finMod$fitted, finMod$residuals, colour = race, data = training)
plot(finMod$residuals, pch = 19)
pred <- predict(modFit, testing)
qplot(wage, pred, colour = year, data = testing)
par(mfrow = c(1,1))
plot(finMod$residuals, pch = 19)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(Hmisc)
str(concrete)
qplot(CompressiveStrength, data = concrete)
plot(concrete$CompressiveStrength, pch = 19)
featurePlot(x = concrete[ , c('Cement', 'BlastFurnaceSlag', 'FlyAsh', 'Water',
'Superplasticizer', 'CoarseAggregate',
'FineAggregate', 'Age')],
y = concrete$CompressiveStrength,
plot = 'pairs')
cutFly <- cut2(taining$FlyAsh, g = 5)
cutFly <- cut2(training$FlyAsh, g = 5)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot'))
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
View(concrete)
plot(concrete$flyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$FlyAsh, pch = 19)
plot(concrete$Age ~ concrete$CompressiveStrength, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot'))
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutBlast <- cut2(training$BlastFurnaceSlag, g = 4)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 5)
table(cutBlast)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 6)
table(cutBlast)
qplot(cutBlast, CompressiveStrength, data = training, fill = cutBlast, geom = c('boxplot', 'jitter'))
cutFly <- cut2(training$FlyAsh, g = 6)
table(cutFly)
cutFly <- cut2(training$FlyAsh, g = 7)
table(cutFly)
qplot(cutFly, CompressiveStrength, data = training, fill = cutFly, geom = c('boxplot', 'jitter'))
cutAge <- cut2(training$Age, g = 6)
table(cutAge)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
cutBlast <- cut2(training$BlastFurnaceSlag, g = 7)
table(cutBlast)
cutAge <- cut2(training$Age, g = 4)
table(cutAge)
qplot(cutAge, CompressiveStrength, data = training, fill = cutAge, geom = c('boxplot', 'jitter'))
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
install.packages("iris")
library(iris)
samConcrete <- concrete[200:300, ]
plot(samConcrete$CompressiveStrength ~ samConcrete$Age, pch = 19)
samConcrete0 <- concrete[100:200, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
plot(concrete$CompressiveStrength ~ concrete$Age, pch = 19)
samConcrete0 <- concrete[1:100, ]
plot(samConcrete0$CompressiveStrength ~ samConcrete0$Age, pch = 19)
samConcrete1 <- concrete[101:200, ]
plot(samConcrete1$CompressiveStrength ~ samConcrete1$Age, pch = 19)
samConcrete2 <- concrete[201:300, ]
plot(samConcrete2$CompressiveStrength ~ samConcrete2$Age, pch = 19)
samConcrete3 <- concrete[301:400, ]
plot(samConcrete3$CompressiveStrength ~ samConcrete3$Age, pch = 19)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
str(training)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer))
hist(training$Superplasticizer, xlim = c(0, 0.0001))
hist(training$Superplasticizer, xlim = c(0, 0.001))
hist(training$Superplasticizer, xlim = c(0, 0.01))
hist(training$Superplasticizer, xlim = c(0, 0.0001), bin = 10)
qplot(training$Superplasticizer)
qplot(training$Superplasticizer, xlim = c(0, 0.001))
qplot(log(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
adData
View(adData)
str(adData)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
which(LETTERS == 'R')
which((1:12) %% 2 == 0)
qt(0.05/2, 7)
source('~/Dropbox/Coursera/Practical Machine Learning/Project/Project.R')
install.packages("rattle")
source('~/.active-rstudio-document')
library(caret)
library(ggplot2)
library(gridExtra)
library(rpart)
library(randomForest)
library(rattle)
library(doMC)
registerDoMC(cores = 4)
# The data is imported from hard drive and into to R memory. The data can also be imported directly from the URL as well.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv")
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv")
# Inspecting the data, several column vectors that are empty and with NA values are found. This can be dealt with by specifying na.strings during import itself.
training <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-training.csv", na.strings = c(NA, '', '#DIV/0!'))
testing <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Project/pml-testing.csv", , na.strings = c(NA, '', '#DIV/0!'))
dim(training); dim(testing)
# Examining the data suggests there are still columns with high percentage of NA values
training <- training[ , (nrow(training) - colSums(is.na(training)))/nrow(training) > 0.9]
testing  <-  testing[ , (nrow(testing) - colSums(is.na(testing)))/nrow(testing) > 0.9]
dim(training); dim(testing)
# Plot the interactions by user, by classe.
grid_arrange_shared_legend <- function(...) {
plots <- list(...)
g <- ggplotGrob(plots[[1]] + theme(legend.position="bottom"))$grobs
legend <- g[[which(sapply(g, function(x) x$name) == "guide-box")]]
lheight <- sum(legend$height)
grid.arrange(
do.call(arrangeGrob, lapply(plots, function(x)
x + theme(legend.position="none"))),
legend,
ncol = 1,
heights = unit.c(unit(1, "npc") - lheight, lheight))
}
plot1 <- ggplot(data = training, aes(x = user_name, y = total_accel_belt)) + geom_point(aes(colour = classe))
plot2 <- ggplot(data = training, aes(x = user_name, y = total_accel_arm)) + geom_point(aes(colour = classe))
plot3 <- ggplot(data = training, aes(x = user_name, y = total_accel_dumbbell)) + geom_point(aes(colour = classe))
plot4 <- ggplot(data = training, aes(x = user_name, y = total_accel_forearm)) + geom_point(aes(colour = classe))
grid_arrange_shared_legend(plot1, plot2, plot3, plot4)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_forearm, y = pitch_forearm )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = training, aes(x = total_accel_dumbbell, y = magnet_dumbbell_z )) + geom_point() + facet_wrap(~classe, nrow =1)
# Removing the columns that will not be used in prediction
myTraining <- training[ , 8:60]
mTesting <- testing[ , 8:60]
dim(training); dim(testing)
# Convert the response variable to a factor variable, to be predicted
training$classe <- factor(training$classe)
# Create models for prediction:
# The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable
# in the training set. You may use any of the other variables to predict with. You should create a report describing
# how you built your model, how you used cross validation, what you think the expected out of sample error is, and why
# you made the choices you did. You will also use your prediction model to predict 20 different test cases.
# caret allows us to specify a trainControl function that can be applied across all model predictions. Lets
# first define the trainControl parameter
# Lets use a 5-fold cross-validation
trnCtrl <- trainControl(method = 'cv',
number = 5,
allowParallel = TRUE,
verboseIter = TRUE)
model1 <- train(classe ~ .,
data = training,
method = 'rf',
trControl = trnCtrl)
model2 <- train(classe ~ .,
data = training,
method = 'knn',
trControl = trnCtrl)
# c("roll_belt", "pitch_belt", "yaw_belt", "gyros_belt_x", "gyros_belt_y", "gyros_belt_z", "",
#  "accel_belt_y", "accel_belt_z", "magnet_belt_x",  "magnet_belt_y", "magnet_belt_z")
#
# pml_write_files = function(x){
#   n = length(x)
#   for(i in 1:n){
#     filename = paste0("problem_id_",i,".txt")
#     write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
#   }
# }
#
# x <- evaluation_data
# x <- x[feature_set[feature_set!='classe']]
# answers <- predict(rf, newdata=x)
#
# answers
#
# pml_write_files(answers)
library(ggplot2)
ggplot(data = training, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
ggplot(data = myTraining, aes(x = total_accel_belt, y = accel_belt_x )) + geom_point() + facet_wrap(~classe, nrow =1)
library(ggplot)
library(ggplot2)
library(knitr)
install.packages("knitr")
qf(0.95, df1 = 19, df2 = 24)
4.405/0.643
qf(0.95, df1 = 2, df2 = 36)
19*3
117-57
60/24
19/2.5
60/21
19/2.857
library(SDSFoundations)
?data
data()
film <- data(FilmData)
str(film)
film <- FilmData
View(film)
fivenum(film$Days)
boxplot(film$Days, main = 'Days in Theaters', xlab = 'All Films', ylab = '# of days')
hist(film$Days)
boxplot(film$Days ~ film$Genre, main = "Days in Theaters", xlab = 'Genrea', ylab = '# of days')
aggregate(Days ~ Genre, data = film)
aggregate(Days ~ Genre, film)
aggregate(Days ~ Genre, film, mean)
?aggregate
aggregate(Days ~ Genre, film, sd)
daysModel <- aov(film$Days ~ film$Genre)
summary(daysModel)
TukeyHSD(daysModel)
library(MASS)
library(ISLR)
### Simple linear regression
names(Boston)
?Boston
plot(medv~lstat,Boston)
View(Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
`###Qualitative predictors
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
OCCPerf = data.frame()
OCCPerf = edit(OCCPerf, factor.mode = "numeric")
X11()
ls
ls()
library(pROC)
install.packages("pROC")
library(pROC)
titanicDF <- read.csv('http://math.ucdenver.edu/RTutorial/titanic.txt',sep='\t')
titanicDF$Title <- ifelse(grepl('Mr ',titanicDF$Name),'Mr',ifelse(grepl('Mrs ',titanicDF$Name),'Mrs',ifelse(grepl('Miss',titanicDF$Name),'Miss','Nothing')))
# load libraries
library(caret)
library(pROC)
titanicDF <- read.csv('http://math.ucdenver.edu/RTutorial/titanic.txt',sep='\t')
?ifelse
install.packages("pROC")
library(pROC)
ls()
tcltk::View(titanicDF)
c(1:6)
install.packages("mlbench")
library(mlbench)
data("Sonar")
library(dplyr)
glimpse(Sonar)
library(caret)
set.seed(998)
inTraining = createDataPartition(Sonar$Class, p = 0.75)
inTraining = createDataPartition(Sonar$Class, p = 0.75, list = FALSE)
View(inTraining)
training = Sonar[ inTraining, ]
testing  = Sonar[-inTraining, ]
fitControl = trainControl(method = "repeatedCV",
number = 10,
repeats = 10) # 10-fold CV, repeated 10 times!
fitControl
gbmFit1 = train(Class ~ ., data = training,
method = 'gbm',
trControl = fitControl,
verbose = FALSE)
gbmFit1
summary(gbmFit1)
gbmGrid = expand.grid(interaction.depth = c(1, 5, 9),
n.trees = (1:30)*50,
shrinkage = 0.1,
n.minobsinnode = 20)
nrow(gbmGrid)
set.seed(825)
gbmGrid
gbmFit2 = train(Class ~ ., data = training,
method = 'gbm',
trControl = fitControl,
tuneGrid = gbmGrid)
summary(gbmFit2)
gbmFit2
trellis.par.set(caretTheme())
plot(gbmFit1)
plot(gbmFit2)
plot(gbmFit2, metric = "Kappa")
?predict
load("~/Dropbox/EdX/The Analytics Edge/Final/Movies/Movies.csv")
Movies <- read.csv("~/Dropbox/EdX/The Analytics Edge/Final/Movies/Movies.csv", stringsAsFactors=FALSE)
View(Movies)
setwd("~/Dropbox/EdX/The Analytics Edge/Final")
summary(Movies)
library(plyr)
library(dplyr)
glimpse(Movies)
train = Movies[Movies$Year < 2010]
train = Movies[Movies$Year < 2010, ]
test  = Movies[!train]
test  = Movies[-train]
test  = Movies[-train,]
test  = Movies[!Movies$Year < 2010, ]
248 + 36
248 + 86
MoviesTrain = Movies[ Movies$Year < 2010, ]
MoviesTest  = Movies[!Movies$Year < 2010, ]
glimpse(Movies)
MoviesLM = lm(Worldwide ~ ., data = MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(MoviesLM)
cor(MoviesTrain$Worldwide, MoviesTrain$Production.Budget)
MoviesLM = lm(Worldwide ~ Runtime + Crime + Horror + Animation + History +
Nominations + Production.Budget,
data = MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(MoviesLM)
MoviesPred = predict(MoviesLM, newdata = MoviesTest)
summary(MoviesPred)
plot(MoviesTest$Worldwide ~ MoviesPred)
MoviesPred$Residual
MoviesPred$Residuals
MoviesPred$SSE
MoviesPred
(MoviesTest$Worldwide - MoviesPred)
(MoviesTest$Worldwide - MoviesPred)^2
sum((MoviesTest$Worldwide - MoviesPred)^2)
sum(MoviesTest$Worldwide - mean(MoviesTrain$Worldwide)^2)
sum((MoviesTest$Worldwide - mean(MoviesTrain$Worldwide))^2)
MoviesFit = lm(MoviesTest$Worldwide ~ MoviesPred)
summary(MoviesFit)
SSE = sum((MoviesTest$Worldwide - MoviesPred)^2)
SST = sum((MoviesTest$Worldwide - mean(MoviesTrain$Worldwide))^2)
1 -(SSE/SST)
Movies$Performance = factor(ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .75), "Excellent",
ifelse(Movies$Worldwide > quantile(Movies$Worldwide, .25), "Average","Poor")))
table(Movies$Performance)
Movies$Worldwide = NULL
library(rpart)
MoviesCART = rpart(Worldwide ~ .,
data = MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(MoviesCART)
library(rpart.plot)
plot(MoviesCART)
text(MoviesCART)
View(Movies)
MoviesCART = rpart(Performance ~ .,
data = MoviesTrain[ , 3:ncol(MoviesTrain)])
summary(MoviesCART)
plot(MoviesCART)
text(MoviesCART)
spl = sample.split(Movies$Performance, SplitRatio = 0.70)
library(caTools)
spl = sample.split(Movies$Performance, SplitRatio = 0.70)
MoviesTrain = subset(Movies, spl = TRUE)
MoviesTest  = subset(Movies, spl = FALSE)
MoviesCART = rpart(Performance ~ .,
data = MoviesTrain[ , 3:ncol(MoviesTrain)])
library(rattle)
rattle()
