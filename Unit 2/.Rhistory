plot(CocaCola$StockPrice ~ CocaCola$Date, pch = 18, col = 'red')
lines(ProcterGamble$Date, ProcterGamble$StockPrice, col = 'blue', lty=2)
abline(v=as.Date(c("2000-03-01")), lwd=2)
abline(v=as.Date(c("1983-01-01")), lwd=1)
plot(CocaCola$Date[301:432], CocaCola$StockPrice[301:432], type="l", col="red", ylim=c(0,210))
abline(v=as.Date(c("2000-03-01")), lwd=1)
lines(ProcterGamble$Date[301:432], ProcterGamble$StockPrice[301:432], col = 'blue')
lines(IBM$Date[301:432], IBM$StockPrice[301:432], col = 'green')
lines(GE$Date[301:432], GE$StockPrice[301:432], col = 'purple')
lines(Boeing$Date[301:432], Boeing$StockPrice[301:432], col = 'black')
abline(v=as.Date(c("1997-09-01")), lwd=1)
abline(v=as.Date(c("1997-11-01")), lwd=1)
months(IBM$Date)
tapply(IBM$StockPrice, months(IBM$Date), mean) - mean(IBM$StockPrice) > 0
which.max(tapply(GE$StockPrice, months(GE$Date), mean))
which.max(tapply(CocaCola$StockPrice, months(CocaCola$Date), mean))
mvt <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/mvtWeek1.csv")
nrow(mvt)
str(mvt)
mvt$ID[which.max(mvt$ID)]
mvt$Beat[which.min(mvt$Beat)]
table(mvt$Arrest)
summary(mvt$LocationDescription == "ALLEY")
DateConvert = as.Date(strptime(mvt$Date, "%m/%d/%y %H:%M"))
summary(DateConvert)
mvt$Month = months(DateConvert)
mvt$Weekday = weekdays(DateConvert)
mvt$Date = DateConvert
arrestsByMonth <- table(mvt$Month, mvt$Arrest)
totalByMonth <- arrestsByMonth[, 1] + arrestsByMonth[, 2]
which.min(totalByMonth)
arrestsByDays <- table(mvt$Weekday, mvt$Arrest)
totalByDays <- arrestsByDays[, 1] + arrestsByDays[, 2]
which.max(totalByDays)
arrests <- subset(mvt, Arrest == TRUE)
arrestsByMonth <- table(arrests$Month, arrests$Arrest)
which.max(arrestsByMonth)
arrestsByMonth[5]
hist(mvt$Date, breaks=100)
boxplot(mvt$Date ~ mvt$Arrest)
which.min(table(mvt$Month))
sum(mvtWeek1$Year==2001 & mvtWeek1$Arrest=="TRUE")/sum(mvtWeek1$Year==2001)
sum(mvt$Year==2007 & mvt$Arrest=="TRUE")/sum(mvt$Year==2007)
sum(mvt$Year==2012 & mvt$Arrest=="TRUE")/sum(mvt$Year==2012)
sum(mvtWeek1$Year==2012 & mvtWeek1$Arrest=="TRUE")/sum(mvtWeek1$Year==2012)
sort(table(mvt$LocationDescription))
Top5 <- subset(mvt, LocationDescription %in% c('STREET',
'PARKING LOT/GARAGE(NON.RESID.)',
'ALLEY',
'GAS STATION',
'DRIVEWAY - RESIDENTIAL'))
View(Top5)
nrow(Top5)
Top5$LocationDescription = factor(Top5$LocationDescription)
arrestLocation <- as.data.frame(table(Top5$LocationDescription, Top5$Arrest))
percentage <- as.data.frame(arrestLocation[ ,3][6:10]*100/(arrestLocation[ ,3][1:5]+ arrestLocation[ ,3][6:10]))
View(percentage)
colnames(arrestPercentage)[1] = 'Location'
arrestLocation[1:5,  1]
arrestPercentage <- data.frame(as.data.frame(arrestLocation[1:5,  1]), percentage)
colnames(arrestPercentage)[1] = 'Location'
colnames(arrestPercentage)[2] = 'Arrest Percentage'
View(arrestPercentage)
gasStation <- subset(Top5, LocationDescription == 'GAS STATION')
which.max(table(gasStation$Weekday))
sort(table(mvt$LocationDescription))
resDriveway <- subset(Top5, LocationDescription == 'DRIVEWAY - RESIDENTIAL')
which.min(table(resDriveway$Weekday))
IBM <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/IBMStock.csv")
GE <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/GEStock.csv")
CocaCola <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/CocaColaStock.csv")
Boeing <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/BoeingStock.csv")
ProcterGamble <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/ProcterGambleStock.csv")
IBM$Date = as.Date(IBM$Date, "%m/%d/%y")
GE$Date = as.Date(GE$Date, "%m/%d/%y")
CocaCola$Date = as.Date(CocaCola$Date, "%m/%d/%y")
ProcterGamble$Date = as.Date(ProcterGamble$Date, "%m/%d/%y")
Boeing$Date = as.Date(Boeing$Date, "%m/%d/%y")
IBM$Date[which.min(IBM$Date)]
companies <- c('IBM',
'GE',
'CocaCola',
'Boeing',
'ProcterGamble')
# Need to work on this!
for (c in companies){
print (c)
}
mean(IBM$StockPrice)
min(GE$StockPrice)
max(CocaCola$StockPrice)
median(Boeing$StockPrice)
sd(ProcterGamble$StockPrice)
plot(CocaCola$StockPrice ~ CocaCola$Date, pch = 18)
CocaCola$Date[which.max(CocaCola$StockPrice)]
CocaCola$Date[which.min(CocaCola$StockPrice)]
plot(CocaCola$StockPrice ~ CocaCola$Date, pch = 18, col = 'red')
lines(ProcterGamble$Date, ProcterGamble$StockPrice, col = 'blue', lty=2)
abline(v=as.Date(c("2000-03-01")), lwd=2)
abline(v=as.Date(c("1983-01-01")), lwd=1)
plot(CocaCola$Date[301:432], CocaCola$StockPrice[301:432], type="l", col="red", ylim=c(0,210))
abline(v=as.Date(c("2000-03-01")), lwd=1)
lines(ProcterGamble$Date[301:432], ProcterGamble$StockPrice[301:432], col = 'blue')
lines(IBM$Date[301:432], IBM$StockPrice[301:432], col = 'green')
lines(GE$Date[301:432], GE$StockPrice[301:432], col = 'purple')
lines(Boeing$Date[301:432], Boeing$StockPrice[301:432], col = 'black')
abline(v=as.Date(c("1997-09-01")), lwd=1)
abline(v=as.Date(c("1997-11-01")), lwd=1)
months(IBM$Date)
tapply(IBM$StockPrice, months(IBM$Date), mean) - mean(IBM$StockPrice) > 0
which.max(tapply(GE$StockPrice, months(GE$Date), mean))
which.max(tapply(CocaCola$StockPrice, months(CocaCola$Date), mean))
IBM$Date[which.max(IBM$Date)]
plot(CocaCola$Date[301:432], CocaCola$StockPrice[301:432], type="l", col="red", ylim=c(0,210))
abline(v=as.Date(c("2000-03-01")), lwd=1)
lines(ProcterGamble$Date[301:432], ProcterGamble$StockPrice[301:432], col = 'blue')
lines(IBM$Date[301:432], IBM$StockPrice[301:432], col = 'green')
lines(GE$Date[301:432], GE$StockPrice[301:432], col = 'purple')
lines(Boeing$Date[301:432], Boeing$StockPrice[301:432], col = 'black')
abline(v=as.Date(c("1997-09-01")), lwd=1)
abline(v=as.Date(c("1997-11-01")), lwd=1)
months(IBM$Date)
tapply(IBM$StockPrice, months(IBM$Date), mean) - mean(IBM$StockPrice) > 0
which.max(tapply(GE$StockPrice, months(GE$Date), mean))
tapply(GE$StockPrice, months(GE$Date), mean) - mean(GE$StockPrice) > 0
tapply(CocaCola$StockPrice, months(CocaCola$Date), mean) - mean(CocaCola$StockPrice) > 0
tapply(GE$StockPrice, months(GE$Date), mean)
which.max(tapply(GE$StockPrice, months(GE$Date), mean))
which.max(tapply(CocaCola$StockPrice, months(CocaCola$Date), mean))
PS <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/CPSData.csv")
nrow(CPS)
str(CPS)
# Among the interviewees with a value reported for the Industry variable, what
# is the most common industry of employment? Please enter the name exactly how you see it.
which.max(table(CPS$Industry))
sort(table(CPS$State))
sum(CPS$Citizenship == 'Citizen, Native' | CPS$Citizenship == 'Citizen, Naturalized')/nrow(CPS)
table(CPS$Race[])
table(CPS$Hispanic)
Hispanic <- subset(CPS, Hispanic == 1)
table(Hispanic$Race) > 250
summary(CPS)
is.na(CPS$Married)
table(CPS$Region, is.na(CPS$Married))
table(CPS$Sex, is.na(CPS$Married))
table(CPS$Age, is.na(CPS$Married))
table(CPS$State, is.na(CPS$MetroAreaCode))
# How many states had all interviewees living in a non-metropolitan area (aka they have a missing MetroAreaCode value)? For this question, treat the District of Columbia as a state (even though it is not technically a state).
sum(table(CPS$State, is.na(CPS$MetroAreaCode))[ ,1] == 0)
# How many states had all interviewees living in a metropolitan area? Again, treat the District of Columbia as a state.
sum(table(CPS$State, is.na(CPS$MetroAreaCode))[ ,2] == 0)
# Which region of the United States has the largest proportion of interviewees living in a non-metropolitan area?
RegProp <- table(CPS$Region, is.na(CPS$MetroAreaCode))
RegProp[ ,2]/(RegProp[ ,1] + RegProp[ ,2])
# Which state has a proportion of interviewees living in a non-metropolitan area closest to 30%?
sort(tapply(is.na(CPS$MetroAreaCode), CPS$State, mean, na.rm = TRUE))
# How many observations (codes for metropolitan areas) are there in MetroAreaMap?
nrow(MetroAreaMap)
nrow(CountryMap)
CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)
str(CPS)
sum(is.na(CPS$MetroArea))
# Which of the following metropolitan areas has the largest number of interviewees?
which.max(table(CPS$MetroArea))
# Which metropolitan area has the highest proportion of interviewees of Hispanic ethnicity?
hispanic <- table(CPS$MetroArea, CPS$Hispanic)
hispanic[,2][1]
which.max(hispanic[ ,2]/(hispanic[ ,1] + hispanic[ ,2]))
# OR use tapply as follows
which.max(tapply(CPS$Hispanic, CPS$MetroArea, mean))
# Determine the number of metropolitan areas in the United States from which at least 20% of interviewees are Asian.
Asian <- table(CPS$MetroArea, CPS$Race == "Asian")
sort(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20)
CPS = merge(CPS, CountryMap, by.x = "CountryOfBirthCode", by.y = "Code", all.x = TRUE)
summary(CPS)
# Determine which metropolitan area has the smallest proportion of interviewees who have received no high school diploma.
which.min(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm = TRUE))
str(CPS)
# How many interviewees have a missing value for the new country of birth variable?
summary(CPS)
#
sort(table(CPS$Country))
# Proportion of the interviewees from the "New York-Northern New Jersey-Long Island,
# NY-NJ-PA" metropolitan area have a country of birth that is not the United States?
notUS <- table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA",
CPS$Country != "United States")
table(CPS$Country != 'United States')
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
table(CPSData$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
notUS
source('~/Dropbox/EdX/The Analytics Edge/Unit 1/Assignment 1.2.R', echo=TRUE)
which.max(table(CPS$Industry))
sort(table(CPS$State))
sum(CPS$Citizenship == 'Citizen, Native' | CPS$Citizenship == 'Citizen, Naturalized')/nrow(CPS)
table(CPS$Race[])
Hispanic <- subset(CPS, Hispanic == 1)
table(Hispanic$Race) > 250
summary(CPS)
is.na(CPS$Married)
table(CPS$Region, is.na(CPS$Married))
summary(CPS)
table(CPS$Region, is.na(CPS$Married))
table(CPS$Sex, is.na(CPS$Married))
table(CPS$Age, is.na(CPS$Married))
table(CPS$State, is.na(CPS$MetroAreaCode))
sum(table(CPS$State, is.na(CPS$MetroAreaCode))[ ,1] == 0)
sum(table(CPS$State, is.na(CPS$MetroAreaCode))[ ,2] == 0)
RegProp <- table(CPS$Region, is.na(CPS$MetroAreaCode))
RegProp[ ,2]/(RegProp[ ,1] + RegProp[ ,2])
sort(tapply(is.na(CPS$MetroAreaCode), CPS$State, mean, na.rm = TRUE))
nrow(MetroAreaMap)
MetroAreaMap <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/MetroAreaCodes.csv")
View(MetroAreaMap)
nrow(MetroAreaMap)
CountryMap <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 1/CountryCodes.csv")
View(CountryMap)
nrow(CountryMap)
CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)
str(CPS)
sum(is.na(CPS$MetroArea))
hispanic <- table(CPS$MetroArea, CPS$Hispanic)
which.max(table(CPS$MetroArea))
table(CPS$MetroArea)
sort(table(CPS$MetroArea))
hispanic <- table(CPS$MetroArea, CPS$Hispanic)
hispanic[,2][1]
which.max(hispanic[ ,2]/(hispanic[ ,1] + hispanic[ ,2]))
which.max(tapply(CPS$Hispanic, CPS$MetroArea, mean))
Asian <- table(CPS$MetroArea, CPS$Race == "Asian")
sort(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20)
sum(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20)
sum(as.numeric(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20))
as.numeric(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20)
as.numeric(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20, na.rm = TRUE)
Asian <- table(CPS$MetroArea, CPS$Race == "Asian", na.rm = TRUE)
sum(as.numeric(Asian[ ,2]/(Asian[ ,1] + Asian[ ,2]) >= 0.20),  na.rm = TRUE)
CPS = merge(CPS, CountryMap, by.x = "CountryOfBirthCode", by.y = "Code", all.x = TRUE)
summary(CPS)
sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean))
tapply(CPS$Education == "No high school diploma")
tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean)
tapply(CPS$Education, CPS$MetroArea, mean)
tapply(CPS$Education, CPS$MetroArea, mean)
str(CPS)
table(CPS$Education, CPS$MetroArea)
tapply(CPS$Education, CPS$MetroArea, mean, na.rm = TRUE)
sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean))
sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean,  na.rm = TRUE))
which.min(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm = TRUE))
str(CPS)
str(CPS)
summary(CPS)
sort(table(CPS$Country))
str(CPS)
sort(table(CPS$Country, CPS$CountryOfBirthCode))
table(CPS$Country)
sort(table(CPS$Country, CPS$CountryOfBirthCode))
sort(table(CPS$Country))
summary(CPS)
sort(table(CPS$Country, CPS$Citizenship))
sort(table(CPS$Citizenship))
sort(table(CPS$Citizenship, CPS$Country))
str(CPS)
sort(table(CPS$Citizenship, CPS$CountryOfBirthCode))
nonCitizens = subset(CPS, Citizenship != "Citizen, Native")
table(nonCitizens$Country)
sort(table(nonCitizens$Country))
otUS <- table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA",
CPS$Country != "United States")
table(CPS$Country != 'United States')
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
table(CPSData$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
notUS
notUS <- table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA",
CPS$Country != "United States")
table(CPS$Country != 'United States')
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
table(CPSData$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA")
notUS
1688/sum(notUS)
table(CPS$Citizenship)
table(CPS$Citizenship, CPS$Country)
table(CPS$Citizenship, CPS$Country)[, 3]
table(CPS$Citizenship, CPS$Country)[3,]
which.max(table(CPS$Citizenship, CPS$Country)[3,])
Sort(table(CPS$Citizenship, CPS$Country)[3,])
sort(table(CPS$Citizenship, CPS$Country)[3,])
sort(CPS$Country)
table(CPS$Country)
sort(table(CPS$Country))
str(CPS)
CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA"
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country)
as.numeric(table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country))
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country)[2,]
sort(table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country)[2,])
tapply(CPS$Country=="United States", CPS$MetroArea=="New York-Northern New Jersey-Long Island, NY-NJ-PA", summary, rm.na=TRUE)
1668/(1668+3736)
tapply(CPS$MetroArea, CPS$Country=="India", summary, rm.na=TRUE)
tapply(CPS$MetroArea, CPS$Country=="Brazil", summary, rm.na=TRUE)
tapply(CPS$MetroArea, CPS$Country=="Somalia", summary, rm.na=TRUE)
setwd("~/Dropbox/EdX/The Analytics Edge/Unit 2")
wine = read.csv("wine.csv")
wine <- read.csv("~/Dropbox/EdX/The Analytics Edge/Unit 2/wine.csv")
str(wine)
summary(wine)
# Linear Regression (one variable)
model1 = lm(Price ~ AGST, data=wine)
summary(model1)
# Sum of Squared Errors
model1$residuals
SSE = sum(model1$residuals^2)
SSE
# Linear Regression (two variables)
model2 = lm(Price ~ AGST + HarvestRain, data=wine)
summary(model2)
# Sum of Squared Errors
SSE = sum(model2$residuals^2)
SSE
# Linear Regression (all variables)
model3 = lm(Price ~ AGST + HarvestRain + WinterRain + Age + FrancePop, data=wine)
summary(model3)
# Sum of Squared Errors
SSE = sum(model3$residuals^2)
SSE
model3.1 = lm(Price ~  HarvestRain + WinterRain, data=wine)
summary(model3.1)
model4 = lm(Price ~ AGST + HarvestRain + WinterRain + Age, data=wine)
summary(model4)
cor(wine$WinterRain, wine$Price)
cor(wine$Age, wine$FrancePop)
cor(wine)
model5 = lm(Price ~ AGST + HarvestRain + WinterRain, data=wine)
summary(model5)
cor(wine$HarvestRain, wine$WinterRain)
wineTest = read.csv("wine_test.csv")
str(wineTest)
# Make test set predictions
predictTest = predict(model4, newdata=wineTest)
predictTest
View(wineTest)
SSE = sum((wineTest$Price - predictTest)^2)
SST = sum((wineTest$Price - mean(wine$Price))^2)
1 - SSE/SST
baseball = read.csv("baseball.csv")
baseball = read.csv("baseball.csv")
str(baseball)
moneyball = subset(baseball, Year < 2002)
str(moneyball)
# Compute Run Difference
moneyball$RD = moneyball$RS - moneyball$RA
str(moneyball)
# Scatterplot to check for linear relationship
plot(moneyball$RD, moneyball$W)
# Regression model to predict wins
WinsReg = lm(W ~ RD, data=moneyball)
summary(WinsReg)
80.881375 + 0.105766*99
str(moneyball)
# Regression model to predict runs scored
RunsReg = lm(RS ~ OBP + SLG + BA, data=moneyball)
summary(RunsReg)
RunsReg = lm(RS ~ OBP + SLG, data=moneyball)
summary(RunsReg)
OBP = 0.361
SLG = 0.500
-804.63 + 2737.77*OBP + 1584.91*SLG
OOBP = 0.297
OSLG = 0.370
-837.38 + 2913.60*OOBP + 1514.29*OSLG
OBP = 0.311
SLG = 0.405
-804.63 + 2737.77*OBP + 1584.91*SLG
teamRank = c(1,2,3,3,4,4,4,4,5,5)
wins2012 = c(94, 88, 95, 88, 93, 94, 98, 97, 93, 94)
wins2013 = c(97, 97, 92, 93, 92, 96, 94, 96, 92, 90)
cor(teamRank, wins2012)
cor(teamRank, wins2013)
climate = read.csv("climate_change.csv")
training_model = subset(climate, Year < 2007)
test_model = subset(climate, Year > 2006)
TempReg = lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data=training_model)
summary(TempReg)
ClimateStepModel = step(TempReg)
cor(climate)
cor(training_model)
TempReg2 = lm(Temp ~ MEI + TSI + Aerosols + N2O, data=training_model)
summary(TempReg2)
# Problem 4 - Automatically Building the Model
# Enter the R2 value of the model produced by the step function
# Which of the following variable(s) were eliminated from the full model by the step function?
ClimateStepModel = step(TempReg)
TempReg3 = lm(Temp ~ MEI + CO2 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data = training_model)
summary(TempReg3)
TempPredictions = predict(ClimateStepModel, newdata=test_model)
SSE = sum((TempPredictions-test_model$Temp)^2)
SST = sum((mean(training_model$Temp)-test_model$Temp)^2)
R2 = 1-SSE/SST
R2
pisaTrain = read.csv("pisa2009train.csv")
pisaTrain = read.csv("pisa2009train.csv")
pisaTest = read.csv("pisa2009test.csv")
str(pisaTrain)
# Problem 1.2 - Summarizing the dataset
# Using tapply() on pisaTrain, what is the average reading test score of males?
tapply(pisaTrain$readingScore, pisaTrain$male, mean)
summary(pisaTrain)
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)
# How many observations are now in the training set?
str(pisaTrain)
str(pisaTest)
table(pisaTrain$grade)
table(pisaTrain)
table(pisTrain$schoolSize)
table(pisaTrain$schoolSize)
table(pisaTrain$readingScore)
table(pisaTrain$raceth)
str(pisaTrain)
table(pisaTrain$raceeth)
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")
lmScore = lm(readingScore ~ ., data=pisaTrain)
summary(lmScore)
SSE = sum(lmScore$residuals^2)
RMSE = sqrt(SSE / nrow(pisaTrain))
sqrt(mean(lmScore$residuals^2))
summary(lmScore)
summary(lmScore)
predTest = predict(lmScore, newdata=pisaTest)
summary(predTest)
637.7-353.2
SSE = sum((predTest-pisaTest$readingScore)^2)
SSE
RMSE = sqrt(SSE/nrow(pisaTest))
RMSE
mean(pisaTrain$readingScore)
SST = sum((pisaTest$readingScore - mean(pisaTrain$readingScore))^2)
SST
R2 = 1 - SSE/SST
R2
source('~/.active-rstudio-document', echo=TRUE)
summary(FluTrain)
sort(tapply(FluTrain$ILI, FluTrain$Week, max))
sort(tapply(FluTrain$Queries, FluTrain$Week, max))
# or
subset(FluTrain, Queries == max(Queries))
subset(FluTrain, ILI == max(ILI))
# Problem 1.2 - Understanding the Data
# What best describes the distribution of values of ILI?
hist(FluTrain$ILI)
# Problem 1.3 - Understanding the Data
# Plot the natural logarithm of ILI versus Queries. What does the plot suggest?
plot(FluTrain$Queries, log(FluTrain$ILI))
# What is the training set R-squared value for FluTrend1 model (the "Multiple R-squared")?
FluTrend1 = lm(log(ILI) ~ Queries, data=FluTrain)
summary(FluTrend1)
# Problem 2.3 - Linear Regression Model
# What is the relationship we infer from our problem?
cor(log(FluTrain$ILI), FluTrain$Queries)
.8420333*.8420333
log(1/.8420333)
FluTest = read.csv("FluTest.csv")
FluTest = read.csv("FluTest.csv")
PredTest1 = exp(predict(FluTrend1, newdata=FluTest))
# What is our estimate for the percentage of ILI-related physician visits for the week of March 11, 2012?
PredTest1[which(FluTest$Week == "2012-03-11 - 2012-03-17")]
# What is the relative error betweeen the estimate (our prediction) and the observed value for the week of March 11, 2012?
(FluTest$ILI[11] - 2.187378)/FluTest$ILI[11]
# What is the Root Mean Square Error (RMSE) between our estimates and the actual observations for the percentage of ILI-related physician visits, on the test set?
SSE = sum((PredTest1-FluTest$ILI)^2)
RMSE = sqrt(SSE / nrow(FluTest))
RMSE
install.packages("zoo")
library(zoo)
ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)
# How many values are missing in the new ILILag2 variable?
sum(is.na(FluTrain$ILILag2))
# or
summary(FluTrain$ILILag2)
# Problem 4.2 - Training a Time Series Model
plot(log(FluTrain$ILILag2), log(FluTrain$ILI))
# There is a strong positive relationship between log(ILILag2) and log(ILI).
# Problem 4.3 - Training a Time Series Model
FluTrend2 = lm(log(ILI) ~ Queries + log(ILILag2), data=FluTrain)
summary(FluTrend2)
# Problem 4.4 - Training a Time Series Model
# FluTrend2 is a stronger model than FluTrend1 on the training set.
# Problem 5
# Problem 5.1 - Evaluating the Time Series Model in the Test Set
ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE)
FluTest$ILILag2 = coredata(ILILag2)
summary(FluTrain$ILILag2)
# Problem 5.2 - Evaluating the Time Series Model in the Test Set
# Which value should be used to fill in the ILILag2 variable for the first observation in FluTest?
# The ILI value of the second-to-last observation in the FluTrain data frame.
# Which value should be used to fill in the ILILag2 variable for the second observation in FluTest?
# The ILI value of the last observation in the FluTrain data frame.
# Problem 5.3 - Evaluating the Time Series Model in the Test Set
# What is the new value of the ILILag2 variable in the first row of FluTest?
# What is the new value of the ILILag2 variable in the second row of FluTest?
FluTest$ILILag2[1] = FluTrain$ILI[nrow(FluTrain)-1]
FluTest$ILILag2[2] = FluTrain$ILI[nrow(FluTrain)]
FluTest$ILILag2[1]
FluTest$ILILag2[2]
# Problem 5.4 - Evaluating the Time Series Model in the Test Set
# What is the test-set RMSE of the FluTrend2 model?
PredTest2 = exp(predict(FluTrend2, newdata=FluTest))
SSE2 = sum((FluTest$ILI - PredTest2)^2)
RMSE2 = sqrt(SSE2/nrow(FluTest))
RMSE2
