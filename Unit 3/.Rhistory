com.data <- data.frame(rf.predict, gbm.predict, lda.predict, testing$diagnosis)
View(com.data)
com.model <- train(diagnosis ~ ., data = com.data, method = 'rf')
com.model <- train(testing.diagnosis ~ ., data = com.data, method = 'rf')
com.predict <- predict(com.model, testing)
rf.predict$overall[1]
gbm.predict$overall[1]
lda.predict$overall[1]
com.predict$overall[1]
com <- confusionMatrix(com.predict, testing$diagnosis)
rf$$overall[1]
rf$overall[1]
lda <- confusionMatrix(lda.predict, testing$diagnosis)
```
rf  <- confusionMatrix( rf.predict, testing$diagnosis)
gbm <- confusionMatrix(gbm.predict, testing$diagnosis)
lda <- confusionMatrix(lda.predict, testing$diagnosis)
rf$overall[1]
rf$overall[1]
gbm$overall[1]
lda$overall[1]
com$overall[1]
gbm$overall[1]
rf$overall[1]
gbm$overall[1]
lda$overall[1]
com$overall[1]
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
com.predict <- predict(com.model, com.data$testing.diagnosis)
com <- confusionMatrix(com.predict, testing$diagnosis)
com$overall[1]
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
?plot.enet
lasso.fit <- train(CompressiveStrength ~ ., data = training, method = "lasso")
?plot.enet
plot.enet(fit$finalModel, xvar = "penalty", use.color = TRUE)
plot.enet(lasso.fit$finalModel, xvar = "penalty", use.color = TRUE)
lasso.fit
plot.enet(lasso.fit$finalModel, xvar = "fraction", use.color = TRUE)
plot.enet(lasso.fit$finalModel, xvar = "step", use.color = TRUE)
library(lubridate)  # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
gaData <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Week4/gaData.csv")
View(gaData)
training = dat[year(dat$date) < 2012,]
library(lubridate)  # For year() function below
gaData =  read.csv("~/Dropbox/Coursera/Practical Machine Learning/Week4/gaData.csv")
training = gaData[year(dat$date) < 2012,]
testing = gaData[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(lubridate)  # For year() function below
gaData <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Week4/gaData.csv")
training <- gaData[year(dat$date) < 2012,]
testing <- gaData[(year(dat$date)) > 2011,]
tstrain <- ts(training$visitsTumblr)
library(lubridate)  # For year() function below
install.packages("lubridate")
library(lubridate)  # For year() function below
gaData <- read.csv("~/Dropbox/Coursera/Practical Machine Learning/Week4/gaData.csv")
training <- gaData[year(dat$date) < 2012,]
training <- gaData[year(gaData$date) < 2012,]
testing <- gaData[(year(gaData$date)) > 2011,]
tstrain <- ts(training$visitsTumblr)
library("forecast", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
library(forecast)
?bats()
bats(tstrain)
plot(forecast(bats(tstrain)))
bats(tstrain, use.parallel = FALSE)
plot(forecast(bats(tstrain)))
plot(bat.model)
bat.model <- bats(tstrain, use.parallel = FALSE)
plot(bat.model)
?forecast
fcast <- forecast(bat.model, level = 95 h = dimension(testing)[1])
fcast <- forecast(bat.model, level = 95, h = dimension(testing)[1])
fcast <- forecast(bat.model, level = 95, h = dim(testing)[1])
plot(fcast)
View(gaData)
plot(x = gaData$date, y = gaData$visitsTumblr)
plot(x = training$date, y = training$visitsTumblr)
?plot
plot(x = training$date, y = training$visitsTumblr, type = 'l')
plot(x = training$date, y = training$visitsTumblr, type = "l")
plot(fcast)
accuracy(fcast, testing$visitsTumblr)
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
source('~/.active-rstudio-document', echo=TRUE)
source('~/Dropbox/Coursera/Practical Machine Learning/Week4/Quiz 4 Q1.R', echo=TRUE)
confusionMatrix(predict1, vowel.test$y)$Accuracy
confusionMatrix(predict2, vowel.test$y)$Accuracy
confusionMatrix(predict1, predict2)$Accuracy
confusionMatrix(predict1, vowel.test$y)[1]
confusionMatrix(predict1, vowel.test$y)[[1]]
confusionMatrix(predict1, vowel.test$y)$overall
confusionMatrix(predict1, vowel.test$y)$overall[1]
confusionMatrix(predict2, vowel.test$y)$overall[1]
confusionMatrix(predict1, predict2)$overall[1]
source('~/.active-rstudio-document', echo=TRUE)
search()
1 - 10^-50
0.9^50
film <- FilmData
library(SDSFoundations)
film <- FilmData
str(film)
hist(film$Gross)
head(film[, film$Gross >1.5E+09])
film[, film$Gross >1.5E+09]
film[ , film$Studio == 'Universal Studios']
table(film$Studio)
film[ , film$Studio == 'Uni.']
film[film$Studio == 'Uni.', ]
head(film[film$Studio == 'Uni.', ])
str(film)
film[ , film$Rank < 10]
View(film)
film[film$Rank < 10, ]
film[film$Rank < 10, ]$IMDB
fivenum(film[film$Rank < 10, ]$IMDB)
str(film)
# Show how many films are in each group
table(film$Rating)
## Question 1
### Calculate avg film budget of each group
aggregate(Budget~Rating,film,mean)
### Calculate sd of film budget within each group
aggregate(Budget~Rating,film,sd)
### Visualize the group means and variability
boxplot(film$Budget~film$Rating, main= "Film Budgets by Rating",
ylab= "Budget", xlab= "MPAA Rating")
### Run ANOVA
modelbud <- aov(film$Budget~film$Rating)
summary(modelbud)
### Run post-hoc test if F statistic is significant
TukeyHSD(modelbud)
# Show how many films are in each group
table(film$Rating)
## Question 1
### Calculate avg film budget of each group
aggregate(Budget~Rating,film,mean)
### Calculate sd of film budget within each group
aggregate(Budget~Rating,film,sd)
### Visualize the group means and variability
boxplot(film$Budget~film$Rating, main= "Film Budgets by Rating",
ylab= "Budget", xlab= "MPAA Rating")
### Run ANOVA
modelbud <- aov(film$Budget~film$Rating)
summary(modelbud)
### Run post-hoc test if F statistic is significant
TukeyHSD(modelbud)
### Calculate avg IMDB score of each group
aggregate(IMDB~Rating,film,mean)
### Calculate sd of IMDB scores within each group
aggregate(IMDB~Rating,film,sd)
### Visualize the group means and variability
boxplot(film$IMDB~film$Rating, main= "IMDB Scores by Rating",
ylab= "IMDB Score", xlab= "MPAA Rating")
### Run ANOVA
modelscore <- aov(film$IMDB~film$Rating)
summary(modelscore)
### Run post-hod text if F statistic is significant
TukeyHSD(modelscore)
qf(0.95, df1 = 2, df2 = 148)
# Show how many films are in each group
table(film$Rating)
## Question 1
### Calculate avg film budget of each group
aggregate(Budget~Rating,film,mean)
### Calculate sd of film budget within each group
aggregate(Budget~Rating,film,sd)
### Visualize the group means and variability
boxplot(film$Budget~film$Rating, main= "Film Budgets by Rating",
ylab= "Budget", xlab= "MPAA Rating")
### Run ANOVA
modelbud <- aov(film$Budget~film$Rating)
summary(modelbud)
### Run post-hoc test if F statistic is significant
TukeyHSD(modelbud)
## Question 2
### Calculate avg IMDB score of each group
aggregate(IMDB~Rating,film,mean)
### Calculate sd of IMDB scores within each group
aggregate(IMDB~Rating,film,sd)
### Visualize the group means and variability
boxplot(film$IMDB~film$Rating, main= "IMDB Scores by Rating",
ylab= "IMDB Score", xlab= "MPAA Rating")
### Run ANOVA
modelscore <- aov(film$IMDB~film$Rating)
summary(modelscore)
### Run post-hod text if F statistic is significant
TukeyHSD(modelscore)
qf(0.95, df1 = 2, df2 = 148)
str(film)
table(film$Studio)
aggregate(Studio ~ Days, film, mean)
aggregate(Days ~ Studio, film, mean)
boxplot(film$Days ~ film$Studio)
modelDays <- aov(film$Days ~ film$Studio)
summary(modelDays)
qf(0.95, df1 = 4, df2 = 146)
TukeyHSD(modelDays)
str(film)
aggregate(Pct.Dom ~ Studio, film, mean)
boxplot(film$Pct.Dom ~ film$Studio)
modelPct <- aov(film$Pct.Dom ~ film$Studio)
summary(modelPct)
qf(0.95, df1 = 4, df2 = 146)
TukeyHSD(modelPct)
aggregate(Days ~ Studio, film, mean)
boxplot(film$Days ~ film$Studio)
modelDays <- aov(film$Days ~ film$Studio)
summary(modelDays)
qf(0.95, df1 = 4, df2 = 146)
TukeyHSD(modelDays)
film$BudgetLevel[film$Budget < 100] <- 'Low-Budget'
film$BudgetLevel[film$Budget >= 100 & film$Budget < 150] <- 'Medium-Budget'
film$BudgetLevel[film$Budget >= 150] <- 'High-Budget'
View(film)
table(film$BudgetLevel)
aggregate(Pct.Dom ~ BudgetLevel, film, mean)
boxplot(film$Pct.Dom ~ film$BudgetLevel)
modelBudget <- aov(film$Pct.Dom ~ film$BudgetLevel)
summary(modelBudget)
TukeyHSD(modelBudget)
qf(0.95, df1 = 2, df2 = 42)
5949.1 - 2387.1
5949.1 - 2387.7
2387.7/2
(2387.7/2)/(3561.4/42)
Section 1    Section 2	Section 3
8	3	1
4	7	2
6	0	7
8	2	6
6	7	5
4	5	0
tickets <- c(8,4,6,8,6,4, 3,7,0,2,7,5, 1,2,7,6,5,0)
sections <- c(NA*18)
sections <- NA*18
sections <- c('one', 'one', 'one', 'one', 'one', 'one',
'two', 'two','two', 'two', 'two', 'two',
'three', 'three', 'three', 'three', 'three', 'three')
ticket.data <- data.frame(tickets, section)
ticket.data <- data.frame(tickets, sections)
ticket.data
modelTicket <- aov(ticket.data$tickets, ticket.date$sections)
modelTicket <- aov(ticket.data$tickets, ticket.data$sections)
ticket.data$sections <- factor(ticket.data$sections )
modelTicket <- aov(ticket.data$tickets, ticket.data$sections)
modelTicket <- aov(ticket.data$tickets ~ ticket.data$sections)
aggregate(ticket.data$tickets ~ ticket.data$sections)
aggregate(ticket.data$tickets ~ ticket.data$sections, ticket.data, mean)
boxplot(ticket.data$tickets ~ ticket.data$sections)
summary(modelTicket )
qf(0.95, 2, 15)
21 + 97.5
2*factorial(4)/(5^5)
factorial(4)
48/(5*5*5*5*5)
2*factorial(4)/factorial(5)
factorial(52)/(factorial(13)^4)
factorial(52)/(factorial(13)^5)
factorial(90)/(factorial(30)^3)
(factorial(90)/(factorial(30)^3))/(factorial(90)/(factorial(30)*factorial(60)))
library(MASS)
library(ISLR)
install.packages("ISLR")
library(ISLR)
### Simple linear regression
names(Boston)
?Boston
plot(medv~lstat,Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
fix(Carseats)
check_for_XQuartz
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
###Writing R functions
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
library(MASS)
library(ISLR)
### Simple linear regression
names(Boston)
?Boston
plot(medv~lstat,Boston)
fit1=lm(medv~lstat,data=Boston)
fit1
summary(fit1)
abline(fit1,col="red")
names(fit1)
confint(fit1)
predict(fit1,data.frame(lstat=c(5,10,15)),interval="confidence")
### Multiple linear regression
fit2=lm(medv~lstat+age,data=Boston)
summary(fit2)
fit3=lm(medv~.,Boston)
summary(fit3)
par(mfrow=c(2,2))
plot(fit3)
fit4=update(fit3,~.-age-indus)
summary(fit4)
### Nonlinear terms and Interactions
fit5=lm(medv~lstat*age,Boston)
summary(fit5)
fit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6)
attach(Boston)
par(mfrow=c(1,1))
plot(medv~lstat)
points(lstat,fitted(fit6),col="red",pch=20)
fit7=lm(medv~poly(lstat,4))
points(lstat,fitted(fit7),col="blue",pch=20)
plot(1:20,1:20,pch=1:20,cex=2)
`###Qualitative predictors
fix(Carseats)
names(Carseats)
summary(Carseats)
fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats)
summary(fit1)
contrasts(Carseats$ShelveLoc)
###Writing R functions
regplot=function(x,y){
fit=lm(y~x)
plot(x,y)
abline(fit,col="red")
}
attach(Carseats)
regplot(Price,Sales)
regplot=function(x,y,...){
fit=lm(y~x)
plot(x,y,...)
abline(fit,col="red")
}
regplot(Price,Sales,xlab="Price",ylab="Sales",col="blue",pch=20)
fix(Carseats)
5/2^6
setwd("~/Dropbox/EdX/The Analytics Edge/Unit 2")
baseball = read.csv("baseball.csv")
str(baseball)
# Subset to only include moneyball years
moneyball = subset(baseball, Year < 2002)
str(moneyball)
# Compute Run Difference
moneyball$RD = moneyball$RS - moneyball$RA
str(moneyball)
# Scatterplot to check for linear relationship
plot(moneyball$RD, moneyball$W)
# Regression model to predict wins
WinsReg = lm(W ~ RD, data=moneyball)
summary(WinsReg)
# VIDEO 3
str(moneyball)
80.881375 + 0.105766*99
str(moneyball)
# Regression model to predict runs scored
RunsReg = lm(RS ~ OBP + SLG + BA, data=moneyball)
summary(RunsReg)
RunsReg = lm(RS ~ OBP + SLG, data=moneyball)
summary(RunsReg)
OBP = 0.311
SLG = 0.405
-804.63 + 2737.77*OBP + 1584.91*SLG
OOBP = 0.297
OSLG = 0.370
-837.38 + 2913.60*OOBP + 1514.29*OSLG
OBP = 0.338
SLG = 0.540
-804.63 + 2737.77*OBP + 1584.91*SLG
1,400,000
1400000/976.5877
OBP = 0.391
SLG = 0.450
-804.63 + 2737.77*OBP + 1584.91*SLG
1065000/979.0476
OBP = 0.369
SLG = 0.374
-804.63 + 2737.77*OBP + 1584.91*SLG
295000/798.3635
OBP = 0.313
SLG = 0.447
-804.63 + 2737.77*OBP + 1584.91*SLG
800000/760.7468
OBP = 0.361
SLG = 0.500
-804.63 + 2737.77*OBP + 1584.91*SLG
300000/976.16
teamRank = c(1,2,3,3,4,4,4,4,5,5)
wins2012 = c(94, 88, 95, 88, 93, 94, 98, 97. 93, 94)
wins2012 = c(94, 88, 95, 88, 93, 94, 98, 97, 93, 94)
wins2013 = c(97, 97, 92, 93, 92, 96, 94, 96, 92, 90)
cor(teamRank, wins2012)
cor(teamRank, wins2013)
# VIDEO 1
# Read in the data
NBA = read.csv("NBA_train.csv")
str(NBA)
View(NBA)
table(NBA$W, NBA$Playoffs)
NBA$PTSdiff = NBA$PTS - NBA$oppPTS
# Check for linear relationship
plot(NBA$PTSdiff, NBA$W)
# Linear regression model for wins
WinsReg = lm(W ~ PTSdiff, data=NBA)
summary(WinsReg)
PointsReg = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + TOV + STL + BLK, data=NBA)
summary(PointsReg)
# Sum of Squared Errors
PointsReg$residuals
SSE = sum(PointsReg$residuals^2)
SSE
# Root mean squared error
RMSE = sqrt(SSE/nrow(NBA))
RMSE
# Average number of points in a season
mean(NBA$PTS)
# Remove insignifcant variables
summary(PointsReg)
PointsReg2 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + DRB + STL + BLK, data=NBA)
summary(PointsReg2)
PointsReg3 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL + BLK, data=NBA)
summary(PointsReg3)
PointsReg4 = lm(PTS ~ X2PA + X3PA + FTA + AST + ORB + STL, data=NBA)
summary(PointsReg4)
# Compute SSE and RMSE for new model
SSE_4 = sum(PointsReg4$residuals^2)
RMSE_4 = sqrt(SSE_4/nrow(NBA))
SSE_4
RMSE_4
NBA_test = read.csv("NBA_test.csv")
# Make predictions on test set
PointsPredictions = predict(PointsReg4, newdata=NBA_test)
# Compute out-of-sample R^2
SSE = sum((PointsPredictions - NBA_test$PTS)^2)
SST = sum((mean(NBA$PTS) - NBA_test$PTS)^2)
R2 = 1 - SSE/SST
R2
# Compute the RMSE
RMSE = sqrt(SSE/nrow(NBA_test))
RMSE
climate = read.csv("climate_change.csv")
str(climate)
Aerosols, data = climate)
climateReg = lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols,
data = climate)
summary(climateReg)
x1 = 1
x2 = 5
b0 = -1.5
b1 = 3
b2 = -0.5
logit = b0 + b1*x1 + b2*x2
logit
exp(logit)
Py1 = 1/(1 + exp(-logit))
py1
Py1
setwd("~/Dropbox/EdX/The Analytics Edge/Unit 3")
